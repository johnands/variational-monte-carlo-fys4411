\documentclass[english, a4paper]{article}

\usepackage[T1]{fontenc}    % Riktig fontencoding
\usepackage[utf8]{inputenc} % Riktig tegnsett
\usepackage{babel}   % Ordelingsregler, osv
\usepackage{graphicx}       % Inkludere bilder
\usepackage{booktabs}       % Ordentlige tabeller
\usepackage{url}            % Skrive url-er
\usepackage{textcomp}       % Den greske bokstaven micro i text-mode
\usepackage{units}          % Skrive enheter riktig
\usepackage{float}          % Figurer dukker opp der du ber om
\usepackage{lipsum}         % Blindtekst
\usepackage{subcaption} 
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath}  
\usepackage{braket} 
\usepackage{multicol}
%\usepackage[]{mcode}

% add source code in box
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{%
  \parbox{\textwidth}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}\vskip-4pt}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\lstset{frame=lrb,xleftmargin=\fboxsep,xrightmargin=-\fboxsep}

\usepackage{amsfonts}
\usepackage{setspace}
\usepackage[cm]{fullpage}		% Smalere marger.
\usepackage{verbatim} % kommentarfelt.
\setlength{\columnseprule}{1pt}	%(width of separationline)
\setlength{\columnsep}{1.0cm}	%(space from separation line)
\newcommand\lr[1]{\left(#1\right)} 
\newcommand\lrb[1]{\left[#1\right]} 
\newcommand\bk[1]{\langle#1\rangle} 
\newcommand\uu[1]{\underline{\underline{#1}}} % Understreker dobbelt.



% JF i margen
\makeatletter
\makeatother
\newcommand{\jf}[1]{\subsubsection*{JF #1}\vspace*{-2\baselineskip}}

% Skru av seksjonsnummerering (-1)
\setcounter{secnumdepth}{3}

\begin{document}
\renewcommand{\figurename}{Figure}
% Forside
\begin{titlepage}
\begin{center}

\textsc{\Large FYS4411}\\[0.5cm]
\textsc{\Large Spring 2016}\\[1.5cm]
\rule{\linewidth}{0.5mm} \\[0.4cm]
{ \huge \bfseries Variational Monte Carlo studies of bosonic systems}\\[0.10cm]
\rule{\linewidth}{0.5mm} \\[1.5cm]

% Av hvem?
\begin{minipage}{0.49\textwidth}
    \begin{center} \large
        John-Anders Stende \\[0.8cm]
    \end{center}
\end{minipage}


\vfill

% Dato nederst
\large{Date: \today}

\end{center}
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{multicols*}{2}

\begin{abstract}
The aim of this project is to use the Variational Monte Carlo (VMC) method to evaluate the 
ground state energy of a trapped, hard sphere Bose gas for different numbers of particles
with a specific trial wave function. 

***Main findings***
\end{abstract}


\section*{Introduction}
Demonstrations of Bose-Einstein condensation (BEC) in gases of alkali atoms confined in magnetic traps has gained
a lot of interest in the scientific community in recent years. Of interest is for instance the fraction of condensed atoms, 
the nature of the condensate and the excitations above the condensate. \\

\noindent An important feature of the trapped alkali systems is that they are dilute, i.e. the effective atom size
is small compared to both the trap size and the inter-atomic spacing. In this situation the physics is
dominated by two-body collisions, well discribed in terms of the $s$-wave scattering length $a$ of the atoms.
The condition for diluteness is defined by the gas parameter $x(\mathbf{r}) = n(\mathbf{r})a^3$, where $n(\mathbf{r})$
is the local density of the system. The theoretical framework of the Gross-Pitaevski equation is valid for $x_{av} \leq 10^{-3}$,
but recent experiments have shown that the gas parameter may exceed this value due to the presence of so-called Feshbach resonance.
Therefore, other methods like the VMC method may be needed. \\

\noindent In this project we evaluate the ground state energy of a trapped BEC by simulating different numbers of bosons
in a harmonic oscillator potential in one, two and three dimensions. The energy is obtained using the VMC method, both with 
and without importance sampling. We have studied both the interacting and the non-interacting case, i.e. with both 
an uncorrelated and a correlated trial wave function. The method of blocking is utilized to do statistical analysis on
the numerical data. We optimize the variational parameter $\alpha$ using the steepest descent method. The one-body 
density in the interacting and non-interacting case is alsox computed. 


\section*{Theory}
The trap we use is a spherical (S) or an elliptical (E) harmonic trap in one, two and three dimensions, with the latter given by
\begin{equation}
 V_{ext}({\bf r}) = 
 \Bigg\{
\begin{array}{ll}
	 \frac{1}{2}m\omega_{ho}^2r^2 & (S)\\
 \strut
	 \frac{1}{2}m[\omega_{ho}^2(x^2+y^2) + \omega_z^2z^2] & (E)
 \label{trap_eqn}
\end{array}
\end{equation}
where $\omega_{ho}$ and $\omega_z$ defines the trap potential strength in the $xy$-plane and $z$-direction respectively.
The two-body Hamiltonian is
 \begin{equation}
     H = \sum_i^N \left(
	 \frac{-\hbar^2}{2m}
	 { \bigtriangledown }_{i}^2 +
	 V_{ext}({\bf{r}}_i)\right)  +
	 \sum_{i<j}^{N} V_{int}({\bf{r}}_i,{\bf{r}}_j),
 \end{equation}
 and we reresent the inter-boson interaction by a pairwise, repulsive potential
 \begin{equation}
 V_{int}(|{\bf r}_i-{\bf r}_j|) =  \Bigg\{
 \begin{array}{ll}
	 \infty & {|{\bf r}_i-{\bf r}_j|} \leq {a}\\
	 0 & {|{\bf r}_i-{\bf r}_j|} > {a}
 \end{array}
 \end{equation}
 where ${a}$ is the so-called hard-core diameter of the bosons.
 
 Our trial wave function for the ground state with N atoms is given by
  \begin{equation}
 \Psi_T({\bf R})=\Psi_T({\bf r}_1, {\bf r}_2, \dots {\bf r}_N,\alpha,\beta)=\prod_i g(\alpha,\beta,{\bf r}_i)\prod_{i<j}f(a,|{\bf r}_i-{\bf r}_j|),
 \label{eq:trialwf}
 \end{equation}
 where $\alpha$ and $\beta$ are variational parameters.
 The single-particle wave function is proportional to the harmonic
 oscillator function for the ground state, i.e.,
 \begin{equation}
    g(\alpha,\beta,{\bf r}_i)= \exp{[-\alpha(x_i^2+y_i^2+\beta z_i^2)]}.
 \end{equation}
 For spherical traps we have $\beta = 1$ and for non-interacting
 bosons ($a=0$) we have $\alpha = 1/2a_{ho}^2$.  The correlation wave
 function is
 \begin{equation}
    f(a,|{\bf r}_i-{\bf r}_j|)=\Bigg\{
 \begin{array}{ll}
	 0 & {|{\bf r}_i-{\bf r}_j|} \leq {a}\\
	 (1-\frac{a}{|{\bf r}_i-{\bf r}_j|}) & {|{\bf r}_i-{\bf r}_j|} > {a}.
 \end{array}
 \end{equation}
 
 
\subsection*{Analytical results}

The quantity we are aiming to compute is the expectiation value of the so-called local energy
 \begin{equation}
    E_L({\bf R})=\frac{1}{\Psi_T({\bf R})}H\Psi_T({\bf R}),
    \label{localenergy}
 \end{equation}
We can find closed-form expressions for the local energy with our specific Hamiltonian $H$ and trial wavefunction $\Psi_T$.
Computing the local energy involves a second derivative of $\Psi_T$, which can be expensive to compute numerically. 
Analytical expressions are therefore useful, as they can speed up the computations.\\

\noindent First, we find the local energy with only the (spherical) harmonic oscillator potential, that is we set $a=0$ and $\beta=1$.
During these calcuations we will use natural units, thus $\hbar=m=1$.
For one particle in one dimension we have
\begin{equation}
\Psi_T(x) = e^{-\alpha x^2}
\end{equation}
and
\begin{equation}
    H =  
	 -\frac{1}{2}
	 \frac{\partial^2}{\partial x^2} +
	 \frac{1}{2}\omega x^2
\end{equation}
The second derivate of the trial wave function is
\begin{equation}
 \frac{\partial^2 \Psi_T}{\partial x^2} = 2\alpha e^{-\alpha x^2}(2\alpha x^2 - 1)
\end{equation}
so that
\begin{equation}
 E_L = \frac{1}{\Psi_T}H\Psi_T = \alpha(1 - 2\alpha x^2) + \frac{1}{2}\omega^2x^2
\end{equation}
In three dimensions the double derivative is replaced by the Laplacian when computing the kinetic energy
\begin{align}
 \bigtriangledown^2\Psi_T &= 2\alpha(2\alpha x^2 - 1) + 2\alpha(2\alpha y^2 - 1) + 2\alpha(2\alpha z^2 - 1) \\
                          &= 2\alpha(2\alpha r^2 - 3)
\end{align}
thus the local energy is
\begin{equation}
 E_L = -\frac{1}{2}\bigtriangledown^2\Psi_T + V_{ext} = \alpha(3 - 2\alpha r^2) + \frac{1}{2}\omega^2r^2 
\end{equation}
We now turn our attention to $N$ particles, with the following wavefunction and Hamiltonian
\begin{align}
 &\Psi_T({\bf R}) = \prod_i e^{-\alpha r_i^2} \\
 &H =     \sum_i^N \left(
	 -\frac{1}{2}
	 { \bigtriangledown }_{i}^2 +
	 \frac{1}{2}\omega^2r_i^2 \right)
\end{align}
The first term of the k-th Laplacian of this wavefunction is
\begin{equation}
 \bigtriangledown^2_k\prod_i e^{-\alpha r_i^2} = 2\alpha(2\alpha x_k^2 - 1)\prod_i e^{-\alpha r_i^2}
\end{equation}
and when we divide with $\Psi_T$ to obtain the the local energy we end up with
\begin{equation}
 E_L = \sum_i^N \left( \alpha(3 - 2\alpha r_i^2) + \frac{1}{2}\omega^2r_i^2 \right)
\end{equation}
For one dimension the expression is
\begin{equation}
 E_L = \sum_i^N \left( \alpha(1 - 2\alpha x_i^2) + \frac{1}{2}\omega^2x_i^2 \right)
\end{equation}
\\

\noindent It is also useful to compute the analytical expression for the drift force $F$ to be used in importance sampling
\begin{equation}
 F = \frac{2\nabla \Psi_T}{\Psi_T}.
\end{equation}
The gradient of $\Psi_T$ is
\begin{align}
 \nabla \Psi_T &= (-2\alpha x, -2\alpha y, -2\alpha z)e^{-\alpha r^2} \\
               &= -2\alpha e^{-\alpha r^2} \bf{r}
\end{align}
Dividing by the wavefunction yields
\begin{equation}
 F = -4\alpha \bf{r}
\end{equation}


Find local energy for the whole system...


\section*{Methods}

We use the \textit{Variational Monte Carlo} (VMC) method in this project to obtain the ground state energy
for our bosonic system. VMC applies the \textit{variational principle} from quantum mechanics
\begin{equation}
 E_0 \leq \frac{\langle \Psi_T | H | \Psi_T \rangle}{\langle \Psi_T | \Psi_T \rangle}
\end{equation}
which states that the ground state energy is always less or equal than the expectation value of our Hamiltonian $H$
for any trial wavefunction $\Psi_T$. VMC consists in choosing a trial wavefunction depending on one or more
variational parameters, and finding the values of these parameters for which the expectation value of the 
energy is the lowest possible. The main challenge is to compute the multidimensional integral
\begin{equation}
 \frac{\langle \Psi_T | H | \Psi_T \rangle}{\langle \Psi_T | \Psi_T \rangle} = 
 \frac{\int d {\bf R} \Psi_T^*({\bf R}, \boldsymbol{\alpha}) H({\bf R}) \Psi_T({\bf R}, \boldsymbol{\alpha})}
       {\int d {\bf R} \Psi_T^*({\bf R}, \boldsymbol{\alpha}) \Psi_T({\bf R}, \boldsymbol{\alpha})}
 \label{multidim}
\end{equation}
where $\bf{R}$ is the positions of all the particles and $\boldsymbol{\alpha}$ is the set of variational parameters.
Traditional integration methods like Gauss-Legendre methods are too computationally expensive, therefore 
other methods are needed.

\subsection*{Monte Carlo integration}

Monte Carlo integration employs a non-deterministic approach to evaluate multidimensional integrals like \eqref{multidim}, or
in general
\begin{equation}
 I = \int_\Omega f({\bf x}) d{\bf x}
\end{equation}
Instead of using an explicit integration scheme, we sample points
\begin{equation}
 {\bf x}_1 \dots {\bf x}_N \in \Omega
\end{equation}
according to some rule. The naive approach, called brute force Monte Carlo, is to use $N$ uniform samples. 
The integral can then be approximated as the average of the function values at these points
\begin{equation}
 I \approx \frac{1}{N} \sum_{i=1}^N f({\bf x}_i)
\end{equation}
The brute force method is however not very efficient, as it samples an equal amount of points in all regions of $\Omega$, 
including those where $f$ is zero. 

\subsection*{Metropolis algorithm}

A more clever approach than the brute force method is to sample points according to the probability distribution (PDF)
defined by $f$. Such a PDF is in general difficult to obtain, thus we can't sample directly from it.
The solution is the Metropolis algorithm, which is a method to obtain random samples from a PDF for which 
direct sampling is difficult. 
These sample values are produced iteratively, with the distribution of the next sample being dependent only on 
the current sample value, thus making the sequence of samples into a Markov chain.
We define ${\bf P}_i^{(n)}$ to be the 
probability for finding the system in state $i$ at step $n$. 
The Metropolis algorithm is as follows:
\begin{itemize}
 \item Sample a possible new state $j$ with some probability $T_{i\rightarrow j}$
 \item Accept the new state with probability $A_{i\rightarrow j}$ and use it as the next sample, or
 recect the new state with probability $1 - A_{i\rightarrow j}$ and use state $i$ as sample again
\end{itemize}
The transition probability $T$ and the acceptance probability $A$ must fulfill the principle of detailed balance
\begin{equation}
 \frac{A_{i\rightarrow j}}{A_{j\rightarrow i}} = \frac{p_i T_{i\rightarrow j}}{p_j T_{j\rightarrow i}}
 \label{detailedbalance}
\end{equation}
which ensures that ${\bf P}_i^{(n\rightarrow \infty)} \rightarrow p_i$, i.e. we end up at the correct 
distribution regardless of what we begin with. \\

\noindent The particles undergo a random walk under the guidance of the Metropolis algorithm. 
Defining the PDF 
\begin{equation}
 P({\bf R}) = \frac{|\Psi_T({\bf R})|^2}{\int |\Psi_T({\bf R})|^2 d{\bf R}}
\end{equation}
and the local energy \eqref{localenergy}, the integral \eqref{multidim} can be rewritten as
\begin{equation}
 \langle E_L \rangle = \int P({\bf R}) E_L({\bf R}) d{\bf R}
\end{equation}
and we see that our problem amounts to finding the expectation value of the local energy $E_L$ on the PDF $P$.
The VMC method approximates this integral as
\begin{equation}
 \langle E_L \rangle \approx \frac{1}{N} \sum_{i=1}^N P({\bf R}_i, \boldsymbol{\alpha}) E_L({\bf R}_i, \boldsymbol{\alpha})
\end{equation}
where $N$ is the number of Monte Carlo cycles and ${\bf R}_i$ is the position of the particles at step $i$. 
The integral $\int |\Psi_T({\bf R})|^2 d{\bf R}$ is very difficult to compute, but the Metropolis algorithm only needs
a \textit{ratio} of probabilities to decide if a move is accepted or not. This can be seen if we rewrite 
\eqref{detailedbalance} as
\begin{equation}
 \frac{p_j}{p_i} = \frac{T_{i\rightarrow j} A_{i\rightarrow j}}{T_{j\rightarrow i} A_{j\rightarrow i}}
 \label{ratio}
\end{equation}
In our case $p_j = P({\bf R}_j)$ and $p_i = P({\bf R}_i)$. 
The simplest form of the Metropolis algorithm, called brute force Metropolis, is to assume that
the transition probability $T_{i\rightarrow j}$ is symmetric, implying that $T_{i\rightarrow j} = T_{j\rightarrow i}$;
the ratio of probabilities \eqref{ratio} thus equals the ratio of acceptance probabilities. 
This leads to a  description of the Metropolis algorithm where we accept or reject a new 
move by calculating the ratio 
\begin{equation}
 w = \frac{|\Psi_T({\bf R}_j)|^2}{|\Psi_T({\bf R}_i)|^2}
 \label{ratio2}
\end{equation}
If $w \geq s$, where $s$ is a random number $s \in [0,1]$, the new position is accepted, else we stay
at the same place.
We now have the full machinery of the Monte Carlo approach to obtain the ground state energy of our bosonic system:
\begin{itemize}
 \item Fix the number of Monte Carlo steps and choose the initial positions ${\bf R}$
       and variational parameters $\boldsymbol{\alpha}$.
       Also set the step size $\Delta {\bf R}$ to be used when moving from ${\bf R}_i$ to ${\bf R}_j$.
 \item Initialize the local energy
 \item Choose a random particle
 \item Calculate a trial position ${\bf R}_j = {\bf R}_i + r \dot \Delta {\bf R}$ where $r$ is a random variable
       $r \in [0,1]$
 \item Use the Metropolis algorithm to accept or reject this move by calculating the ratio \eqref{ratio2}. 
       If $w \geq s$, where $s$ is a random number $s \in [0,1]$, the new position is accepted, else we stay
       at the same place.
 \item If the step is accepted, set ${\bf R} = {\bf R}_j$ for the chosen particle
 \item Update the local energy
\end{itemize}
When the Monte Carlo sampling is finished, we calculate the mean local energy, which is our approximation
of the ground state energy of the system.
The Metropolis algorithm is implemented as follows:
\belowcaptionskip=-10pt
\begin{lstlisting}[label=MetropolisBrute,caption=Brute Forde Metropolis algorithm]
int particle = Random::nextInt(m_numberOfParticles);    // choose random particle
int dimension = Random::nextInt(m_numberOfDimensions);  // choose random dimension
double change = (Random::nextDouble()*2-1)*m_stepLength;  // propose change

// get old wavefunction
double waveFuncOld = m_waveFunction->evaluate(m_particles);

// adjust position
m_particles[particle]->adjustPosition(change, dimension);

// get new wavefunction
double waveFuncNew = m_waveFunction->evaluate(m_particles);

// accept/reject new position using Metropolis algorithm
double ratio = pow(waveFuncNew, 2) / pow(waveFuncOld, 2);

if (ratio >= Random::nextDouble()) {
    //cout << m_particles[particle]->getPosition()[0] << endl;
    return true;
}
else {
    // correct position change
    m_particles[particle]->adjustPosition(-change, dimension);
    return false;
}
\end{lstlisting}


\subsection*{Steepest descent method}

We turn now to the problem of finding the variational parameters that minimizes the expectation value
of the local energy $\langle E_L({\bf R}, \alpha) \rangle$. This project considers a trial wavefunction with only one 
variational parameter $\alpha$.
There are many optimization algorithms to choose from, we have chosen the Steepest 
descent method due to its simplicity. This method finds a local minimum of a function by taking steps proportional 
to the negative gradient of the function at a given point, i.e. where the function has the steepest descent.
The algorithm is as follows:
\begin{itemize}
 \item Choose an initial $\alpha_0$ and step length $\gamma_0$.
 \item For $i \geq 0$: Compute $\alpha_{i+1} = \alpha_i - \gamma_i \frac{d \langle E_L({\bf R}, \alpha) \rangle}{d\alpha}$
 \item Continue until a maximum number of steps are performed or $|\alpha_{i+1} - \alpha_{i}|$ is 
       less than some tolerance
\end{itemize}
$\langle E_L({\bf R}, \alpha) \rangle$ is as we have seen a multidimensional integral, and the derivative w.r.t. $\alpha$
is not easily computed. Let us define
\begin{equation}
 \bar{E}_\alpha = \frac{d \langle E_L(\alpha) \rangle}{d\alpha}
\end{equation}
and 
\begin{equation}
 \bar{\Psi}_T = \frac{d \Psi_T(\alpha)}{d\alpha}
\end{equation}
Using the chain rule and the hermicity of the Hamiltonian it can be shown that
\begin{equation}
 \bar{E}_\alpha = 2 \left( \Bigr\langle \frac{\bar{\Psi}_T}{\Psi_T(\alpha)} E_L(\alpha) \Bigr\rangle 
 - \Bigr\langle \frac{\bar{\Psi}_T}{\Psi_T(\alpha)}\Bigr\rangle \langle E_L(\alpha) \rangle   \right)
 \label{localenergyalpha}
\end{equation}
thus we need the expectation values of
\begin{equation}
 \frac{\bar{\Psi}_T}{\Psi_T(\alpha)} E_L(\alpha)
 \label{waveenergy}
\end{equation}
and 
\begin{equation}
 \frac{\bar{\Psi}_T}{\Psi_T(\alpha)}
 \label{wavederivative}
\end{equation}
The complete VMC method then amounts to the following:
\begin{itemize}
 \item Make initial guess $\alpha_0$
 \item Run $10^4$-$10^5$ Metropolis steps, sample \eqref{waveenergy} and \eqref{wavederivative}
 \item Compute \eqref{localenergyalpha} 
 \item Compute new $\alpha$ using the Steepest descent method
\end{itemize}
The above steps are repeated until the value of $\alpha$ is sufficiently accurate, before a new round of
Metropolis steps are run, this time with many cycles ($10^6$-$10^8$). We then obtain our approximation
for the ground state energy of the system.\\

\noindent The Steepest descent method is implemented as follows:
\belowcaptionskip=-10pt
\begin{lstlisting}[label=steepestdescent,caption=The Steepest Descent method]
 void SteepestDescent::optimize(double initialAlpha) {

    int maxNumberOfSteps = 30;
    double tolerance = 0.001;
    double oldAlpha = initialAlpha;
    for (int i=0; i < maxNumberOfSteps; i++) {

        // make initial state
        m_system->getInitialState()->setupInitialState();

        // set value of alpha
        m_system->getWaveFunction()->setAlpha(oldAlpha);

        // run metropolis steps
        m_system->runMetropolisSteps((int) 1e4, false, false, false);

        // compute derivative of exp. value of local energy w.r.t. alpha
        double localEnergyDerivative = 2 * 
                             ( m_system->getSampler()->getWaveFunctionEnergy() -
                               m_system->getSampler()->getWaveFunctionDerivative() *
                               m_system->getSampler()->getEnergy() );

        cout << "localEnergyDerivative = " << localEnergyDerivative << endl;

        // compute new alpha
        double newAlpha = oldAlpha - m_stepLengthOptimize*localEnergyDerivative;
        cout << "newAlhpa = " << newAlpha << endl;
        cout << "oldAlpha = " << oldAlpha << endl;
        cout << std::abs(newAlpha-oldAlpha) << endl;
        if ( std::abs(newAlpha - oldAlpha) < tolerance ) {
            break;
        }
        // before new iteration
        oldAlpha = newAlpha;
    }
    cout << "Optimal alpha = " << oldAlpha << endl;

    // run many Metropolis steps with the optimal alpha

    // make initial state
    m_system->getInitialState()->setupInitialState();

    // set value of alpha
    m_system->getWaveFunction()->setAlpha(oldAlpha);

    // run metropolis steps
    m_system->runMetropolisSteps((int) 1e7, false, false, false);
}
\end{lstlisting}


\subsection*{Importance sampling}

A more efficient way to 


\subsection*{Blocking}

Monte Carlo simulations can be treated as computer experiments. The results can be analyzed with the same
statistical tools as we would use analyzing experimental data. We are looking for expectation values
of these data, and how accurate they are.
A stochastic process like a Monte Carlo experiment produces sequentially a chain of values
\begin{equation}
 \{x_1, x_2 \dots x_k \dots x_n \}
\end{equation}
called a sample. Each value $x_k$ is called a measurement. The sample variance
\begin{equation}
 var(x) = \frac{1}{n} \sum_{k=1}^n (x_k - \bar{x}_n)
 \label{samplevariance}
\end{equation}
where $\bar{x}_n$ is the sample mean, is a measure of the statistical error of a \textit{uncorrelated} sample.
However, a Monte Carlo simulation produces a correlated sample, thus we need another measure of the sample error.
The sample covariance
\begin{equation}
 cov(x) \equiv \frac{1}{n} \sum_{kl} (x_k - \bar{x}_n) (x_l - \bar{x}_n)
\end{equation}
is a measure of the sequential correlation between succeding measurements of a sample.
(Note that these are experimental values for the sample, not the \textit{true} properties of
the stochastic variables, which we need an infinite number of measurements to calculate).

\noindent It can be shown that an estimate of the error $err_X$ 
of a correlated sample is
\begin{equation}
 err_X = \frac{1}{n} cov(x)
\end{equation}
With the help of the \textit{autocorrelation function} from statistical theory we can rewrite
this error as
\begin{equation}
 err_X = \frac{\tau}{n} var(x)
\end{equation}
where $\tau$ is the \textit{autocorrelation time} which accounts
for the correlation between measurements. In the presence of
correlation the effective number of measurements becomes
\begin{equation}
 n_{eff} = \frac{n}{\tau}
\end{equation}
Neglecting $\tau$ thus gives an error estimate that is less than the true sample error. 
The autocorrelation time is however expensive to compute. We can avoid the computation of this quantity by using
the technique of blocking. The idea behind this method is to split the sample into blocks, find the mean of each block
and then calculate the total mean and variance of all the block means.
This is done for increasing block sizes until the measurements of two sequential blocks are uncorrelated, which
are governed by the autocorrelation time $\tau$\\

\noindent The blocking algorithm is as follows:
\begin{itemize}
 \item Do a Monte Carlo simulation, store the local energy for each step to file
 \item Read the file into an array
 \item Loop over increasing block sizes:
 \begin{itemize}
     \item For each block size $n_b$, loop over array in steps of $n_b$ taking the mean of elements
           $[in_b, (i+1)n_b] , \dots $
     \item Calculate total mean and variance of all block means and store
 \end{itemize}
 \item Plot total variance for all block sizes. 
\end{itemize}

According to the above discussion, the variance will increase for increasing block size until
we reach a plateau. A more correct measure of the sample error is thus obtained. 








\subsection*{Implementation}

The project code is split into different classes that handle different things. We will here give an overview
of what the different classes do and how they communicate. \\

\noindent The main program sets all the parameters needed to a simulatio. 


\section*{Results}

We investigate two different systems, both with only one variational parameter $\alpha$:
\begin{enumerate}
 \item $N=1$, $N=10$, $N=100$ and $N=500$ bosons in one, two and three dimensions in a
       spherical harmonic oscillator potential with no interaction.       
 \item $N=10$, $N=50$ and $N=100$ bosons in three dimensions
       in an elliptical harmonic oscillator potential including interaction and a
       correlated trial wavefunction. 
\end{enumerate}

We calculate the ground state energy for system 1 both with and without importance sampling.
Only the brute force Metropolis algorithm is applied for system 2.
The kinetic energy is calculated both numerically and analytically in both cases.\\

\noindent We use blocking to analyze the error of the statistical data for both systems.
The steepest descent method is applied to optimize $\alpha$. 
In addition to this, the one-body density is computed for both systems.


\subsection*{System 1 - brute force Metropolis}

\begin{lstlisting}
    int numberOfSteps       = (int) 1e4;
    double omega            = 1.0;          // oscillator frequency
    double alpha            = 0.5;          // variational parameter 1
    double stepLength       = 1.5;          // metropolis step length
    double equilibration    = 0.1;          
\end{lstlisting}

Acceptance rate: 0.55

\begin{table}[H]
  \centering
  \begin{tabular}{ | c | r | r | r | r |}
    \hline
    N& D& E& $\sigma$& Time \\*
    \hline
    1& 1& 0.5& 0& 5.53e-3 \\*
    \hline
    1& 2& 1& 0&  5.66e-3\\*
    \hline
    1& 3& 1.5& 0&  5.70e-3\\*
    \hline
    10& 1& 5& 0&  7.20e-3\\*
    \hline
    10& 2& 10& 0&  7.36e-3\\*
    \hline
    10& 3& 15& 0&  1.00-3\\*
    \hline
    100& 1& 50& 0&  0.0215\\*
    \hline
    100& 2& 100& 0&  0.223\\*
    \hline
    100& 3& 150& 0&  0.274\\*
    \hline
    500& 1& 200& 0&  0.0843\\*
    \hline
    500& 2& 500& 0&  0.110\\*
    \hline
    500& 3& 750& 0&  0.131\\*
    \hline
  \end{tabular}
  \caption{Analytical}
  \label{tab:Tabell1}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{ | c | r | r | r | r |}
    \hline
    N& D& E& $\sigma$& Time \\*
    \hline
    1& 1& 0.49& 9.87e-8& 6.96e-3 \\*
    \hline
    1& 2& 0.99& 1.873-7&  0.0151\\*
    \hline
    1& 3& 1.49& 1.88e-7&  0.0157\\*
    \hline
    10& 1& 4.99& 4.42e-7&  0.0314\\*
    \hline
    10& 2& 9.99& 6.78e-7&  0.0598\\*
    \hline
    10& 3& 14.9& 1.11e-6&  0.0955\\*
    \hline
    100& 1& 49.9& 2.78e-6&  0.743\\*
    \hline
    100& 2& 99.9& 6.18e-6&  1.78\\*
    \hline
    100& 3& 149.9& 8.95e-6&  3.16\\*
    \hline
    500& 1& 199.9& 1.32e-5&  15.1\\*
    \hline
    500& 2& 499.9& 2.52e-5&  41.5\\*
    \hline
    500& 3& 749.9& 6.65e-5&  74.5\\*
    \hline
  \end{tabular}
  \caption{Numerical}
  \label{tab:Tabell1}
\end{table}

From quantum mechanics we know that the exact ground state energy for this system is
\begin{equation}
 E = \frac{\omega N}{2}
\end{equation}


\subsection*{System 1 - importance sampling}

\begin{lstlisting}
    int numberOfSteps       = (int) 1e4;
    double omega            = 1.0;          // oscillator frequency
    double alpha            = 0.5;          // variational parameter 1
    double stepLength       = 1.5;          // metropolis step length
    double equilibration    = 0.1;          // amount of the total steps
    double timeStep         = 0.001;        // importance sampling
\end{lstlisting}

Acceptance rate: 0.9

\begin{table}[H]
  \centering
  \begin{tabular}{ | c | r | r | r | r |}
    \hline
    N& D& E& $\sigma$& Time \\*
    \hline
    1& 1& 0.5& 0& 0.0183 \\*
    \hline
    1& 2& 1& 0&  0.0338\\*
    \hline
    1& 3& 1.5& 0&  0.0501\\*
    \hline
    10& 1& 5& 0&  0.0479\\*
    \hline
    10& 2& 10& 0&  0.0488\\*
    \hline
    10& 3& 15& 0&  0.0812\\*
    \hline
    100& 1& 50& 0&  0.0605\\*
    \hline
    100& 2& 100& 0&  0.1212\\*
    \hline
    100& 3& 150& 0&  0.218\\*
    \hline
    500& 1& 200& 0&  0.171\\*
    \hline
    500& 2& 500& 0&  0.451\\*
    \hline
    500& 3& 750& 0&  0.941\\*
    \hline
  \end{tabular}
  \caption{Analytical}
  \label{tab:Tabell1}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{ | c | r | r | r | r |}
    \hline
    N& D& E& $\sigma$& Time \\*
    \hline
    1& 1& 0.5& 9.41e-8& 0.0170 \\*
    \hline
    1& 2& 1& 1.48e-7&  0.0352\\*
    \hline
    1& 3& 1.5& 1.58e-7&  0.0618\\*
    \hline
    10& 1& 5& 3.42e-7&  0.0413\\*
    \hline
    10& 2& 10& 3.50e-7&  0.104\\*
    \hline
    10& 3& 15& 9.23e-7&  0.163\\*
    \hline
    100& 1& 50& 3.02e-6&  0.737\\*
    \hline
    100& 2& 100& 5.72e-6&  1.889\\*
    \hline
    100& 3& 150& 6.88e-6&  3.319\\*
    \hline
    500& 1& 200& 1.01e-5&  15.509\\*
    \hline
    500& 2& 500& 4.94e-5&  41.047\\*
    \hline
    500& 3& 750& 2.42e-5&  75.584\\*
    \hline
  \end{tabular}
  \caption{Numerical}
  \label{tab:Tabell1}
\end{table}


Dependence on importance sampling time step:\\
N = 100, D = 3


\begin{table}[H]
  \centering
  \begin{tabular}{ | c | r | r |}
    \hline
    Time step& Acceptance rate& $\sigma$ \\*
    \hline
    0.001& 0.871& 0 \\*
    \hline
    0.005& 0.831& 0 \\*
    \hline
    0.01&  0.801& 0 \\*
    \hline
  \end{tabular}
  \caption{Analytical}
  \label{tab:Tabell1}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{ | c | r | r |}
    \hline
    Time step& Acceptance rate& $\sigma$ \\*
    \hline
    0.001& 0.871& 6.88e-6 \\*
    \hline
    0.005& 0.831& 7.39e-6 \\*
    \hline
    0.01&  0.801& 9.91e-6 \\*
    \hline
  \end{tabular}
  \caption{Numerical}
  \label{tab:Tabell1}
\end{table}









































 

























 %\end{multicols*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

\begin{comment}

% deloppgave
\begin{enumerate}
\item[\bf a)]
\item[\bf b)]
\item[\bf c)]
\item[\bf d)]
\end{enumerate}

%%%%%%%%
% Tabell
\begin{table}[H]
  \centering
  \begin{tabular}{ | c | r | r | r | r | r |}
    \hline
    & & & & & \\*
    \hline
    & & & & & \\*
    \hline
  \end{tabular}
  \caption{some caption}
  \label{tab:Tabell1}
\end{table}

%%%%%%%%
% Enkel figur
\begin{figure}[H]
\begin{center}
  \includegraphics[width = 120mm]{/users/filiphl/Desktop/Studie/Emne/ObligX/filnavn.png}
  \caption{some caption}\label{fig:fig1}
  \end{center}
\end{figure}

%%%%%%%%
% 2 figurer sbs
\begin{figure}
\begin{minipage}[t]{0.48\linewidth}
  \includegraphics[width=\textwidth]{fil}
  \caption{}
  \label{fig:minipage1}
\end{minipage}
\quad
\begin{minipage}[t]{0.48\linewidth}
\includegraphics[width=\textwidth]{fil}
  \caption{}
  \label{fig:minipage1}
\end{minipage}
\end{figure}

%%%%%%%%
% X antall kollonner
\begin{multicols*}{X}
\begin{spacing}{0.7} % verticale mellomrom
%kan f.eks benytte align?
\end{spacing}
\end{multicols*}


%%%%%%%%
%Matrise
\begin{equation*}
    {\bf A} = \left(\begin{array}{cccccc}
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                      \end{array} \right)
\end{equation*}
%%%%%%%%

