\documentclass[english, a4paper]{article}

\usepackage[T1]{fontenc}    % Riktig fontencoding
\usepackage[utf8]{inputenc} % Riktig tegnsett
\usepackage{babel}   % Ordelingsregler, osv
\usepackage{graphicx, caption}       % Inkludere bilder
\usepackage{booktabs}       % Ordentlige tabeller
\usepackage{url}            % Skrive url-er
\usepackage{textcomp}       % Den greske bokstaven micro i text-mode
\usepackage{units}          % Skrive enheter riktig
\usepackage{float}          % Figurer dukker opp der du ber om
\usepackage{lipsum}         % Blindtekst
\usepackage{subcaption} 
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath}  
\usepackage{braket} 
\usepackage{multicol}
\usepackage{tikz}
\usepackage[bookmarks]{hyperref}
%\usepackage[]{mcode}

% add source code in box
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{%
  \parbox{\textwidth}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}\vskip-4pt}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\lstset{frame=lrb,xleftmargin=\fboxsep,xrightmargin=-\fboxsep}

\usepackage{amsfonts}
\usepackage{setspace}
\usepackage[cm]{fullpage}		% Smalere marger.
\usepackage{verbatim} % kommentarfelt.
\setlength{\columnseprule}{1pt}	%(width of separationline)
\setlength{\columnsep}{1.0cm}	%(space from separation line)
\newcommand\lr[1]{\left(#1\right)} 
\newcommand\lrb[1]{\left[#1\right]} 
\newcommand\bk[1]{\langle#1\rangle} 
\newcommand\uu[1]{\underline{\underline{#1}}} % Understreker dobbelt.



% JF i margen
\makeatletter
\makeatother
\newcommand{\jf}[1]{\subsubsection*{JF #1}\vspace*{-2\baselineskip}}

% Skru av seksjonsnummerering (-1)
\setcounter{secnumdepth}{3}

\begin{document}
\renewcommand{\figurename}{Figure}
% Forside
\begin{titlepage}
\begin{center}

\textsc{\Large FYS4411}\\[0.5cm]
\textsc{\Large Spring 2016}\\[1.5cm]
\rule{\linewidth}{0.5mm} \\[0.4cm]
{ \huge \bfseries Variational Monte Carlo studies of electronic systems}\\[0.10cm]
\rule{\linewidth}{0.5mm} \\[1.5cm]

% Av hvem?
\begin{minipage}{0.49\textwidth}
    \begin{center} \large
        John-Anders Stende \\[0.8cm]
    \end{center}
\end{minipage}


\vfill

% Dato nederst
\large{Date: \today}

\end{center}
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{multicols*}{2}

\begin{abstract}
The aim of this project is to use the Variational Monte Carlo (VMC) method to evaluate the
ground state energy, onebody densities, expectation values of the kinetic and potential energies
and single-particle energies of quantum dots with $N=2$, $N=6$, $N=12$ and $N=20$ electrons,
i.e. closed-shell systems. A performance analysis of the code is also made.

***Main findings*** 


\end{abstract}

\tableofcontents


\section{Introduction}
\noindent Quantum dots are nano-scale semiconductor devices that contain strongly confined electrons. 
They exhibit discrete quantum levels due to their small size, including shell structures and magic
numbers for the ground states, as in atoms and nuclei. The electronic properties of these materials 
can be tuned by applying external fields, and are thus of interest in many research applications such as
transistors, solar cells, LEDs etc. Studies of quantum dots containing several electrons require
reliable many-body methods that also incorporate uncertainty quantifications.\\

\noindent In this project we compute the ground state energy for
$N=2$, $N=6$, $N=12$ and $N=20$ electrons
confined in a two-dimensional harmonic oscillator trap with different oscillator frequencies $\omega$.
These values of $N$ are magic numbers for the system.
For the two-body case we find the energy both with and without the use of Slater determinants for 
benchmarking purposes. The one-body density is computed for all $N$, both with and without correlations
in the trial wave function. 
For $N=2$ we also compute the mean distance between the electrons and the expectation values of the potential
and kinetic energies. We also perform a timing analysis by comparing a serial and parallel code both with 
and without vectorization.\\

\noindent ***structure of report***



\section{Methods}

We use the \textit{Variational Monte Carlo} (VMC) method in this project to obtain the ground state energy
for our fermonic system. VMC applies the \textit{variational principle} from quantum mechanics
\begin{equation}
 E_0 \leq \frac{\langle \Psi_T | H | \Psi_T \rangle}{\langle \Psi_T | \Psi_T \rangle}
\end{equation}
which states that the ground state energy is always less or equal than the expectation value of the Hamiltonian $H$
for any trial wavefunction $\Psi_T$. VMC consists in choosing a trial wavefunction depending on one or more
variational parameters, and finding the values of these parameters for which the expectation value of the 
energy is the lowest possible. The main challenge is to compute the multidimensional integral
\begin{equation}
 \frac{\langle \Psi_T | H | \Psi_T \rangle}{\langle \Psi_T | \Psi_T \rangle} = 
 \frac{\int d {\bf R} \Psi_T^*({\bf R}, \boldsymbol{\alpha}) H({\bf R}) \Psi_T({\bf R}, \boldsymbol{\alpha})}
       {\int d {\bf R} \Psi_T^*({\bf R}, \boldsymbol{\alpha}) \Psi_T({\bf R}, \boldsymbol{\alpha})}
 \label{multidim}
\end{equation}
where $\bf{R}$ is the positions of all the particles and $\boldsymbol{\alpha}$ is the set of variational parameters.
Traditional integration methods like Gauss-Legendre methods are too computationally expensive, therefore 
other methods are needed.

\subsection{Monte Carlo integration}

Monte Carlo integration employs a non-deterministic approach to evaluate multidimensional integrals like 
\eqref{multidim}, or in general
\begin{equation}
 I = \int_\Omega f({\bf x}) d{\bf x}
\end{equation}
Instead of using an explicit integration scheme, we sample points
\begin{equation}
 {\bf x}_1 \dots {\bf x}_N \in \Omega
\end{equation}
according to some rule. The most naive approach is to use $N$ uniform samples. 
The integral can then be approximated as the average of the function values at these points
\begin{equation}
 I \approx \frac{1}{N} \sum_{i=1}^N f({\bf x}_i)
\end{equation}
This simple approach is however not very efficient, as it samples an equal amount of points in all regions of $\Omega$, 
including those where $f$ is zero. 

\subsection{Metropolis algorithm}

A more clever approach is to sample points according to the probability distribution (PDF)
defined by $f$. Such a PDF is in general difficult to obtain, thus we can't sample directly from it.
Instead we use the Metropolis algorithm, which is a method to obtain random samples from a PDF for which 
direct sampling is difficult. 
These sample values are produced iteratively, with the distribution of the next sample being dependent only on 
the current sample value, thus making the sequence of samples into a Markov chain.
We define ${\bf P}_i^{(n)}$ to be the 
probability for finding the system in state $i$ at step $n$. 
The Metropolis algorithm is as follows:
\begin{itemize}
 \item Sample a possible new state $j$ with some probability $T_{i\rightarrow j}$
 \item Accept the new state with probability $A_{i\rightarrow j}$ and use it as the next sample, or
 recect the new state with probability $1 - A_{i\rightarrow j}$ and use state $i$ as sample again
\end{itemize}
The transition probability $T$ and the acceptance probability $A$ must fulfill the principle of detailed balance
\begin{equation}
 \frac{A_{i\rightarrow j}}{A_{j\rightarrow i}} = \frac{p_i T_{i\rightarrow j}}{p_j T_{j\rightarrow i}}
 \label{detailedbalance}
\end{equation}
which ensures that ${\bf P}_i^{(n\rightarrow \infty)} \rightarrow p_i$, i.e. we end up at the correct 
distribution regardless of what we begin with. \\

\noindent The particles undergo a random walk under the guidance of the Metropolis algorithm. 
Defining the PDF 
\begin{equation}
 P({\bf R}, \boldsymbol{\alpha}) = \frac{|\Psi_T({\bf R},\boldsymbol{\alpha})
 |^2}{\int |\Psi_T({\bf R}, \boldsymbol{\alpha})|^2 d{\bf R}}
\end{equation}
and the local energy,
\begin{equation}
    E_L({\bf R, \boldsymbol{\alpha}})=\frac{1}{\Psi_T({\bf R, \boldsymbol{\alpha}})}H
    \Psi_T({\bf R}, \boldsymbol{\alpha}),
    \label{localEnergy}
 \end{equation}
the integral \eqref{multidim} can be rewritten as
\begin{equation}
 \langle E_L \rangle = \int P({\bf R}, \boldsymbol{\alpha}) E_L({\bf R}, \boldsymbol{\alpha}) d{\bf R}
 \label{localEnergyExp}
\end{equation}
and we see that our problem amounts to finding the expectation value of the local energy $E_L$ on the PDF $P$.
Using Monte Carlo integration, we approximate this integral as
\begin{equation}
 \langle E_L \rangle \approx \frac{1}{N} \sum_{i=1}^N P({\bf R}_i, \boldsymbol{\alpha}) E_L({\bf R}_i, 
 \boldsymbol{\alpha})
\end{equation}
where $N$ is the number of Monte Carlo cycles and ${\bf R}_i$ is the position of the particles at step $i$. 
The integral $\int |\Psi_T({\bf R}, \boldsymbol{\alpha})|^2 d{\bf R}$ is in general very difficult to compute, 
but the Metropolis algorithm only needs
a \textit{ratio} of probabilities to decide if a move is accepted or not. This can be seen if we rewrite 
\eqref{detailedbalance} as
\begin{equation}
 \frac{p_j}{p_i} = \frac{T_{i\rightarrow j} A_{i\rightarrow j}}{T_{j\rightarrow i} A_{j\rightarrow i}}
 \label{ratio}
\end{equation}
In our case $p_j = P({\bf R}_j)$ and $p_i = P({\bf R}_i)$. 
The simplest form of the Metropolis algorithm, called brute force Metropolis, is to assume that
the transition probability $T_{i\rightarrow j}$ is symmetric, implying that $T_{i\rightarrow j} = T_{j\rightarrow i}$;
the ratio of probabilities \eqref{ratio} thus equals the ratio of acceptance probabilities. 
This leads to a  description of the Metropolis algorithm where we accept or reject a new 
move by calculating the ratio 
\begin{equation}
 w = \frac{|\Psi_T({\bf R}_j)|^2}{|\Psi_T({\bf R}_i)|^2}
 \label{metropolisRatio}
\end{equation}
If $w \geq s$, where $s$ is a random number $s \in [0,1]$, the new position is accepted, else we stay
at the same place.
We now have the full machinery of the Monte Carlo approach to obtain the ground state energy of our bosonic system:
\begin{itemize}
 \item Fix the number of Monte Carlo steps and choose the initial positions ${\bf R}$
       and variational parameters $\boldsymbol{\alpha}$.
       Also set the step size $\Delta {\bf R}$ to be used when moving from ${\bf R}_i$ to ${\bf R}_j$.
 \item Initialize the local energy
 \item Choose a random particle
 \item Calculate a trial position ${\bf R}_j = {\bf R}_i + r  \Delta {\bf R}$ where $r$ is a random variable
       $r \in [0,1]$
 \item Use the Metropolis algorithm to accept or reject this move by calculating the ratio 
       \eqref{metropolisRatio}. 
       If $w \geq s$, where $s$ is a random number $s \in [0,1]$, the new position is accepted, else we stay
       at the same place.
 \item If the step is accepted, set ${\bf R} = {\bf R}_j$ for the chosen particle
 \item Sample the local energy
\end{itemize}
When the Monte Carlo sampling is finished, we calculate the mean local energy, which is our approximation
of the ground state energy of the system.
The Metropolis algorithm is implemented as follows:


\subsection{Importance sampling}

A more efficient way to do Monte Carlo sampling is to replace the brute force Metropolis algorithm
with a walk in coordinate space biased by the trial wavefunction. This approach is based on the 
Fokker-Planck equation and the Langevin equation for generating a trajectory in coordinate space. \\

\noindent The Langevin equation is a stochastic differential equation
\begin{equation}
 \frac{\partial x(t)}{\partial t} = D F(x(t)) + \eta
 \label{Langevin}
\end{equation}
where $D$ is the diffusion constant and $\eta$ a random variable.
The new positions $y$ in coordinate space are the solutions of \eqref{Langevin} using Euler's method:
\begin{equation}
 y = x + DF(x)\Delta t + \xi \sqrt{\Delta t}
 \label{LangevinSolution}
\end{equation}
where $\xi$ is a gaussian random variable and $\Delta t$ is a chosen time step. $D$ is equal to $1/2$
which comes from the factor $1/2$ in the kinetic energy operator. $\Delta t$ is to be viewed as a
parameter which yields stable values of the ground state energy for values $\Delta t \in [0.001, 0.01]$.
\eqref{LangevinSolution} is similar to the brute force Metropolis equation for updating positions except for the
term  containing $F(x)$. This is the function that pushes the particles towards regions of configuration space
where the wavefunction is large, in contrast to the brute force method where all regions are equally probable. 
In three dimension $F(x)$ is called the \textit{drift vector} ${\bf F}({\bf x})$.
The drift vector can be found from the
Fokker-Planck equation
\begin{equation}
 \frac{\partial P}{\partial t} = \sum_i D \frac{\partial}{\partial {\bf x}_i}
 \left( {\bf x}_i - {\bf F}_i \right) P({\bf x}, t)
 \label{FokkerPlanck}
\end{equation}
The convergence to a stationary probability density can be obtained by setting the left hand side to zero. 
The resulting equation is only satisfied if all terms of the sum are equal to zero, 
\begin{equation}
 \frac{\partial^2 P}{\partial {\bf x}_i^2} = P \frac{\partial}{\partial {\bf x}_i} {\bf F}_i
 + {\bf F}_i \frac{\partial}{\partial {\bf x}_i} P
 \label{stationaryFokker}
\end{equation}
The drift vector should have the form ${\bf F} = g({\bf x}) \frac{\partial P}{\partial {\bf x}}$. Inserting this in
\eqref{stationaryFokker} yields 
\begin{equation}
 {\bf F} = 2 \frac{1}{\Psi_T} \nabla \Psi_T
 \label{quantumForce}
\end{equation}
which is known as the \textit{quantum force}. \\

\noindent The Monte Carlo method with the Metropolis algorithm can be seen as isotropic diffusion process 
by a time-dependent 
probability density, with or without a drift, corresponding to brute force and importance samling respectively.
The Fokker-Planck equation \eqref{FokkerPlanck} describes such a diffusion process, our
new transition probabilty is thus the solution to this equation, given by the Green's function
\begin{equation}
 G(y, x, \Delta t) = \frac{1}{(4\pi D \Delta t)^{3N/2}}
 \textrm{exp}(-(y - x - D\Delta t F(x))^2 / 4D\Delta t)
\end{equation}
which in turn means that our brute force Metropolis accept/reject ratio \eqref{metropolisRatio} is replaced by
the so-called \text{Metropolis-Hastings} article
\begin{equation}
 q(y, x) = \frac{G(x, y, \Delta t)|\Psi_T(y)|^2}{G(y, x, \Delta t)|\Psi_T(x)|^2}
 \label{metropolisHastings}
\end{equation}
The Metropolis Hastings algorithm is the same as the brute force method, now with \eqref{metropolisRatio} 
replaced by 
\eqref{metropolisHastings} and trial positions
calculated according to \eqref{LangevinSolution}.




\subsection{Steepest descent method}

We turn now to the problem of finding the variational parameters that minimizes the expectation value
of the local energy $\langle E_L({\bf R}, \boldsymbol{\alpha}) \rangle$. This project considers a trial wavefunction
with two variational parameters $\alpha$ and  $\beta$. 
There are many optimization algorithms to choose from, we have chosen the Steepest 
descent method due to its simplicity. This method finds a local minimum of a function by taking steps proportional 
to the negative gradient of the function at a given point, i.e. where the function has the steepest descent.
The algorithm is as follows:
\begin{itemize}
 \item Choose an initial set of parameters $\boldsymbol{\alpha}_0$ and step length $\gamma_0$.
 \item For $i \geq 0$: Compute $\boldsymbol{\alpha}_{i+1} = \boldsymbol{\alpha}_i - \gamma_i 
 \nabla_{\boldsymbol{\alpha}_i} \langle E_L({\bf R}, \boldsymbol{\alpha}_i) \rangle$
 \item Continue until a maximum number of steps are performed or 
 $|\nabla_{\boldsymbol{\alpha}} \langle E_L({\bf R}, \boldsymbol{\alpha}) \rangle|$ is
       less than some tolerance
\end{itemize}
We should get $|\nabla_{\boldsymbol{\alpha}_i}\langle E_L({\bf R}, 
\boldsymbol{\alpha}_i) \rangle|^2 \geq |\nabla_{\boldsymbol{\alpha}_{i+1}}\langle 
E_L({\bf R}, \boldsymbol{\alpha}_{i+1}) \rangle|^2 \geq \dots$.
If this is not the case, we reject the new step and decrease the step size to obtain a more
accurate value. 
$\langle E_L({\bf R}, \boldsymbol{\alpha}) \rangle$ is as we have seen a multidimensional integral 
\eqref{localEnergyExp}, 
and the gradient w.r.t. $\boldsymbol{\alpha}$
is not easily computed.
Let us define
\begin{equation}
 \bar{E}_{c_n} = \frac{\partial \langle E_L(\boldsymbol{\alpha}) \rangle}{\partial c_n}
\end{equation}
where $c_n$ is a variational parameter.
Using the chain rule and the hermicity of the Hamiltonian,
it can be shown that
\begin{equation}
 \bar{E}_{c_n} = 2 \left( \Bigr\langle \frac{\partial \ln{\Psi_T}}{\partial c_n} E_L 
 \Bigr\rangle 
 - \Bigr\langle \frac{\partial \ln{\Psi_T}}{\partial c_n}\Bigr\rangle \langle E_L \rangle   \right)
 \label{localEnergyParameters}
\end{equation}
thus we need the expectation values of
\begin{equation}
 \frac{\partial \ln{\Psi_T}}{\partial c_n} E_L
 \label{sampleOptimize1}
\end{equation}
and 
\begin{equation}
\frac{\partial \ln{\Psi_T}}{\partial c_n}
\label{sampleOptimize2}
\end{equation}
For our trial wave function \eqref{trialWF}, we have
\begin{equation}
 \frac{\partial\ln{\Psi_T}}{\partial c_n} = \frac{\partial\ln{|D|_\uparrow}}{\partial c_n} + 
 \frac{\partial\ln{|D|_\downarrow}}{\partial c_n} + \frac{\partial\ln{\Psi_C}}{\partial c_n}
\end{equation}
Inserting our variational parameters $\alpha$ and $\beta$, we observe that
\begin{equation}
 \frac{\partial\ln{\Psi_C}}{\partial \alpha} = 0
\end{equation}
and 
\begin{equation}
 \frac{\partial\ln{|D|_\uparrow}}{\partial \beta} = \frac{\partial\ln{|D|_\downarrow}}{\partial \beta} = 0
\end{equation}
so we end up with
\begin{equation}
 \frac{\partial\ln{\Psi_T}}{\partial \alpha} = \frac{\partial\ln{|D|_\uparrow}}{\partial \alpha} + 
 \frac{\partial\ln{|D|_\downarrow}}{\partial \alpha}
\end{equation}
and 
\begin{equation}
\frac{\partial\ln{\Psi_T}}{\partial \beta} = \frac{\partial\ln{\Psi_C}}{\partial \beta}
\end{equation}
We can calculate the above derivatives with the help of the following linear algebra identity:
If $A$ is an invertible matrix which depends on a real parameter $t$, and if $dA/dt$ exists, then
\begin{equation}
 \frac{d}{dt}|A| = |A|\textrm{tr}\left(A^{-1}\frac{dA}{dt}\right)
\end{equation}
thus we have
\begin{equation}
 \frac{d}{dt}\ln{|A|(t)} = \textrm{tr}\left(A^{-1}\frac{dA}{dt}\right) = 
 \sum_{i=1}^N\sum_{j=1}^N\frac{dA_{ij}}{dt}A_{ji}^{-1}
\end{equation}
For the Slater determinants, this expression is analogous to the gradient-ratio \eqref{gradientRatioBefore},
only now we differentiate the single-particle wave functions w.r.t. $\alpha$:
\begin{equation}
 \frac{\partial\ln{|D|}}{\partial \alpha} = 
 \sum_{i=1}^{N/2} \sum_{j=1}^{N/2} \frac{\partial \phi_j({\bf R}_i)}{\partial \alpha} D_{ji}^{-1}({\bf R}_i)
\end{equation}
The corresponding expression for the correlation function is
\begin{equation}
 \frac{\partial\ln{\Psi_C}}{\partial \beta} = 
 \sum_{i=1}^N\sum_{j>i}^N \frac{\partial}{\partial\beta}
 \left(\frac{a_{ij}r_{ij}}{1 + \beta r_{ij}}\right) = 
 -\sum_{i=1}^N\sum_{j>i}^N \frac{a_{ij}r_{ij}^2}{(1 + \beta r_{ij})^2}
\end{equation}


\noindent The complete VMC method then amounts to the following:
\begin{itemize}
 \item Make initial guess $\boldsymbol{\alpha}_0$
 \item Run $10^4$-$10^5$ Metropolis steps, sample \eqref{sampleOptimize1} and \eqref{sampleOptimize2}
 \item Compute \eqref{localEnergyParameters} 
 \item Calculate new $\boldsymbol{\alpha}$ using the Steepest descent method
\end{itemize}
The above steps are repeated until the above stopping condition is fulfilled, before a new round of
Metropolis steps are run, this time with many cycles ($10^6$-$10^8$). We then obtain our approximation
for the ground state energy of the system.\\

\noindent The Steepest descent method is implemented as follows:


\subsection{Blocking}

Monte Carlo simulations can be treated as computer experiments. The results can be analyzed with the same
statistical tools as we would use analyzing experimental data. We are looking for expectation values
of these data, and how accurate they are.
A stochastic process like a Monte Carlo experiment produces sequentially a chain of values
\begin{equation}
 \{x_1, x_2 \dots x_k \dots x_N \}
\end{equation}
called a sample. Each value $x_k$ is called a measurement. The sample variance
\begin{equation}
 \textrm{var}(x) = \frac{1}{N} \sum_{k=1}^n (x_k - \bar{x}_N)
 \label{samplevariance}
\end{equation}
where $\bar{x}_N$ is the sample mean, is a measure of the statistical error of a \textit{uncorrelated} sample.
However, a Monte Carlo simulation with interacting particles produces a correlated sample, 
thus we need another measure of the sample error.
It can be shown that an estimate of the error $err_X$ 
of a correlated sample is
\begin{equation}
 \textrm{err}_X = \frac{1}{N} \textrm{cov}(x)
\end{equation}
where $\textrm{cov(x)}$  is the sample covariance
\begin{equation}
 \textrm{cov}(x) \equiv \frac{1}{N} \sum_{kl} (x_k - \bar{x}_N) (x_l - \bar{x}_N)
 \label{samplecovariance}
\end{equation}
which is a measure of the sequential correlation between succeding measurements of a sample.
(Note that \eqref{samplevariance} and \eqref{samplecovariance}
are experimental values for the sample, not the \textit{true} properties of
the stochastic variables, which we need an infinite number of measurements to calculate).
With the help of the \textit{autocorrelation function} from statistical theory we can rewrite
this error as
\begin{equation}
 \textrm{err}_X = \frac{\tau}{N} \textrm{var}(x)
\end{equation}
where $\tau$ is the \textit{autocorrelation time} which accounts
for the correlation between measurements. In the presence of
correlation the effective number of measurements becomes
\begin{equation}
 n_{\textrm{eff}} = \frac{N}{\tau}
\end{equation}
Neglecting $\tau$ thus gives an error estimate that is less than the true sample error. 
The autocorrelation time is however expensive to compute. We can avoid the computation of this quantity by using
the technique of blocking. The idea behind this method is to split the sample into blocks, find the mean of each block
and then calculate the total mean and variance of all the block means.
This is done for increasing block sizes $n_b$ until the measurements of two sequential blocks are uncorrelated,
enabling us to extract the value of $\tau = n_b\Delta t$. The true sample error,
\begin{equation}
 \sigma = \left( \frac{1 + 2\tau / \Delta t}{N} \bigr( \langle E_L^2\rangle - \langle E_L \rangle^2\bigr) \right)^{1/2}
 \label{trueSampleError}
\end{equation}
can then be calculated. \\


\noindent The blocking algorithm is as follows:
\begin{itemize}
 \item Do a Monte Carlo simulation, store the local energy for each step to file
 \item Read the file into an array
 \item Loop over increasing block sizes:
 \begin{itemize}
     \item For each block size $n_b$, loop over array in steps of $n_b$ taking the mean of elements
           $[in_b, (i+1)n_b] , \dots $
     \item Calculate total mean and variance of all block means and store
 \end{itemize}
 \item Plot total variance for all block sizes. 
 \item Extract $\tau$ and compute \eqref{trueSampleError}
\end{itemize}

\section{Systems}
As mentioned in the introduction we want to study electrons confined in a Harmonic Oscillator (HO) 
trap in two dimensions. The trap we will use 
is an isotropic HO potential, with an idealized Hamiltonian given by
\begin{equation}
 H = \sum_{i=1}^N\left(-\frac{1}{2} \nabla_i^2 + \frac{1}{2} \omega^2 r_i^2 \right) + \sum_{i<j} \frac{1}{r_{ij}}
 \label{fullHamiltonian}
\end{equation}
where natural units ($\hbar = c = e = m_e = 1$) are used and all energies are in atomic units a.u. 
The Hamiltonian includes a standard HO part
\begin{equation}
 H_0 = \sum_{i=1}^N \left( -\frac{1}{2}\nabla_i^2 + \frac{1}{2}\omega^2 r_i^2 \right)
 \label{HOHamiltonian}
\end{equation}
and a repulsive interaction between two electrons, given by
\begin{equation}
 H_1 = \sum_{i < j} \frac{1}{r_{ij}}
 \label{repulsiveHamiltonian}
\end{equation}
where $r_{ij} = |{\bf r}_1 - {\bf r}_2|$ is the distance between the electrons. 
The modulus of the positions of the electrons is defined as $r_i = \sqrt{r_{i_x}^2 + r_{i_y}^2}$.


\subsection{Unperturbed system}
If we only include the HO part of the Hamiltonian we say that the system is unperturbed since 
there are no interaction between the electrons. The wave function for one 
electron in an oscillator potential in two dimensions is
\begin{equation}
 \phi_{n_x,n_y}(x,y) = AH_{n_x}(\sqrt{\alpha\omega}x)H_{n_y}
 (\sqrt{\alpha\omega}y)\exp\left(-\frac{\alpha\omega(x^2+y^2)}{2}\right).
 \label{singleParticleWF}
\end{equation}
The functions $H_{n_x}(\sqrt{\alpha\omega}x)$ are Hermite polynomials \eqref{hermite}
while $A$ is a normalization constant. 
For the lowest lying state, $n_x = n_y = 0$, the energy is given by
$\epsilon_{n_x,n_y} = \omega(n_x+n_y+1) = \omega$. Now, due the Pauli exclusion principle we can not 
have two electrons in the same state, however each energy level (distinct $(n_x,n_y)$ pair)
can contain two electrons since they can have either spin up or spin down. We can visualize this as follows:
\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}
  \draw[->] (-3.5,-1) -- (3.5,-1) node[anchor=west] {};
  \draw[->] (-3.5,-1) -- (-3.5,3) node[anchor=south] {$\epsilon_{n_x,n_y}$};

  \draw (-3.6,0) node[anchor=east] {$1\omega$} -- (-3.4,0);
  \draw (-3.6,1) node[anchor=east] {$2\omega$} -- (-3.4,1);
  \draw (-3.6,2) node[anchor=east] {$3\omega$} -- (-3.4,2);

  

  \begin{scope}
    \draw (-.5,0) -- (.5,0);
    \draw[->] (-.25,-.25) node[anchor=north]{} -- (-.25,.25);
    \draw[<-] (.25,-.25)  node[anchor=north]{} -- (.25,.25);
  \end{scope}

  \begin{scope}[xshift=-1cm,yshift=1cm]
    \draw (-.5,0) -- (.5,0);
    \draw[->] (-.25,-.25) node[anchor=north]{} -- (-.25,.25);
    \draw[<-] (.25,-.25) node[anchor=north]{} -- (.25,.25);
  \end{scope}


  \begin{scope}[xshift=1cm,yshift=1cm]
    \draw (-.5,0) -- (.5,0);
    \draw[->] (-.25,-.25) node[anchor=north]{} -- (-.25,.25);
    \draw[<-] (.25,-.25) node[anchor=north]{} -- (.25,.25);
  \end{scope}


  \begin{scope}[xshift=-2cm,yshift=2cm]
    \draw (-.5,0) -- (.5,0);
    \draw[->] (-.25,-.25) node[anchor=north]{} -- (-.25,.25);
    \draw[<-] (.25,-.25) node[anchor=north]{} -- (.25,.25);
  \end{scope}


  \begin{scope}[xshift=0cm,yshift=2cm]
    \draw (-.5,0) -- (.5,0);
    \draw[->] (-.25,-.25) node[anchor=north]{} -- (-.25,.25);
    \draw[<-] (.25,-.25) node[anchor=north]{} -- (.25,.25);
  \end{scope}


  \begin{scope}[xshift=2cm,yshift=2cm]
    \draw (-.5,0) -- (.5,0);
    \draw[->] (-.25,-.25) node[anchor=north]{} -- (-.25,.25);
    \draw[<-] (.25,-.25) node[anchor=north]{} -- (.25,.25);
  \end{scope}

\end{tikzpicture}
\caption{Fig text}
\end{center}
\end{figure}
If we "fill" each energy level with electrons we say that we have a closed shell system, where the number of particles needed to fill each energy 
level constitutes a magic number. The first magic numbers are $N=2,6,12,20$. In the unperturbed case the ground state energy will just be the sum of each individual particle's
energy.
\begin{table}[H] 
  \begin{center}
    \begin{tabular*}{4cm}{c @{\extracolsep{\fill}} c}
      \toprule
      $N$ & $E$ \\ 
      \hline
      2  & $2\omega$ \\
      6  & $10\omega$ \\ 
      12 & $28\omega$ \\ 
      20 & $60\omega$ \\ 
      \bottomrule
      \end{tabular*} 
    \end{center}
    \captionsetup{width=12cm}
      \caption {Exact ground state energies $E$ in atomic units for $N$ electrons in a pure two-dimensional 
                harmonic oscillator with oscillator frequency $\omega$.
                The values of $N$ are magic numbers for the system.} 
  \label{tab:HOEnergies} 
\end{table}
This provides an excellent way to check that our program is working properly when we exclude interaction between the electrons.



\subsection{Perturbed system}
Next we want to describe the perturbed system. 
We study electrons, which are fermions, thus they obey the Pauli exclusion principle.
This means that we have to approximate the exact wave function for the system with a trial
wave function that is antisymmetric. Our antisymmetric ansatz $\Psi_T$ is a product of a 
Slater determinant $\Psi_{SD} = |D|$ and a linear Padé-Jastrow correlation term $\Psi_C$ 
\begin{equation}
 \Psi_C = \exp{\left(\frac{a r_{ij}}{1+\beta r_{ij}}\right)}
 \label{correlationFunction}
\end{equation}
The full wave function for $N$ electrons can be written
\begin{equation}
   \psi_{T}({\bf r}_1,{\bf r}_2,\dots, {\bf r}_N, \alpha, \beta) = \Psi_{SD}\Psi_C = 
   |D({\bf r}_1,{\bf r}_2,\dots, {\bf r}_N, \alpha)|
   \prod_{i<j}^{N}\exp{\left(\frac{a r_{ij}}{1+\beta r_{ij}}\right)}, 
   \label{trialWF}
\end{equation}
where $\alpha$ and $\beta$ are the variational parameters.
The Slater matrix $D$ is defined as
\begin{equation}
 D_{ij} = \phi_j({\bf r}_i)
 \label{slaterMatrix}
\end{equation}
where $\phi_j({\bf r}_i)$ are the single-particle wave functions \eqref{singleParticleWF}, 
i.e. eigenfunctions of the unperturbed
Hamiltonian \eqref{HOHamiltonian}. The rows correspond to the position of a given particle, 
while the columns stand for the various quantum numbers.

\subsubsection{Two-electron perturbed system}
For $N=2$ the trial wave function reduces to
\begin{equation*}
 \Psi_T({\bf r}_1,{\bf r}_2) = C\exp(-\alpha \omega (r_1^2+r_2^2)/2)\exp\left(\frac{ar_{12}}{1+\beta r_{12}}\right).
 \label{trialWF2}
\end{equation*}
In this case we have analytical expressions for the ground state energy for selected oscillator
frequencies which we will use as a way to validate the program. In appendix \ref{app:appendixA1} we show
that the local energy in this case is given by
\begin{equation}
 E_L = 
\end{equation}
We also need the quantum force when we use importance sampling,
\begin{equation}
 F =
\end{equation}

\subsubsection{6-,12- and 20-electron perturbed system}

\section{Optimization of trial wave function and its deriviatives}
\label{sec:optimization}

The trial wave function \eqref{trialWF} plays a central role in our VMC simulation.
It is needed in the Metropolis algorithm and in the evaluation of the quantum force \eqref{quantumForce}.
Moreover, all observables like the local energy \eqref{localEnergy} is computed w.r.t. it. 
The most time-consuming part of the evaluation of the wave function is the computation of the Slater determinant. 
Computing a determinant of an $ N \times N$ matrix by standard Gaussian elimination is of the order of
$\mathcal{O}(N^3)$ calculations. As there are $N \cdot d$ independent coordinates we need to 
evaluate $Nd$ Slater determinants for the gradient (quantum force and kinetic energy) and $Nd$ for 
the Laplacian (kinetic energy). Therefore, it is imperative to find alternative ways of computing
the quantities related to the trial wave function to improve performance. \\

\subsection{Optimization of determinant-to-determinant ratio}
The ratio \eqref{metropolisRatio} used in the Metropolis algorithm is the following 
for our trial wave function (not squared):
\begin{equation}
 R = \frac{|D|^{\textrm{new}}}{|D|^{\textrm{old}}}
 \frac{\Psi_C^{\textrm{new}}}{\Psi_C^{\textrm{old}}}
 \label{metropolisRatioSlater}
\end{equation}
where we label the Slater determinant-to-determinant ratio as $R_{SD}$ and the correlation-to-correlation
ratio $R_C$.
It turns out that we can compute $R_{SD}$ using an algorithm that requires to keep track of the \textit{inverse}
of the Slater matrix \eqref{slaterMatrix}. The inverse of $D$ can be expressed in terms of its cofactors 
$C_{ij}$ and its determinant $|D|$,
\begin{equation}
 D_{ij}^{-1} = \frac{C_{ji}}{|D|}
\end{equation}
where $C_{ji}$ is the transposed cofactor matrix. 
The Slater part $R_{SD}$ of the ratio \eqref{metropolisRatioSlater} can thus be written,
\begin{equation}
 R_{SD} = \frac{|D({\bf R}^{\textrm{new}})|}{|D({\bf R}^{\textrm{old}})|}
  = \frac{\sum_{j=1}^N D_{ij}({\bf R}^{\textrm{new}}) C_{ij}({\bf R}^{\textrm{new}})}
  {\sum_{j=1}^N D_{ij}({\bf R}^{\textrm{old}}) C_{ij}({\bf R}^{\textrm{old}})}
\end{equation}
When moving \textit{one} particle for each Monte Carlo cycle, ${\bf R}^{\textrm{new}}$ differs from 
${\bf R}^{\textrm{old}}$ by the position of only one, say the $i$-th particle. This means that
only the $i$-th row of $D({\bf R}^{\textrm{new}})$
and $D({\bf R}^{\textrm{old}})$ will be different. Taking into account that the $i$-th row of a cofactor matrix
$C$ is independent of the entries of the $i$-th row of its corresponding matrix $D$, we have that
\begin{equation}
 C_{ij}({\bf R}^{\textrm{new}}) = C_{ij}({\bf R}^{\textrm{old}}) \quad j \in \{1,\dots,N\}
\end{equation}
and
\begin{equation}
 R_{SD} = \frac{\sum_{j=1}^N D_{ij}({\bf R}^{\textrm{new}}) D_{ji}^{-1}({\bf R}^{\textrm{old}})}
  {\sum_{j=1}^N D_{ij}({\bf R}^{\textrm{old}}) D_{ji}^{-1}({\bf R}^{\textrm{old}})}
\end{equation}
By definition, the denominator of this expression is unity, thus we obtain for the ratio,
\begin{equation}
 R_{SD} = \sum_{j=1}^N D_{ij}({\bf R}^{\textrm{new}}) D_{ji}^{-1}({\bf R}^{\textrm{old}})
        = \sum_{j=1}^N \phi_j({\bf R}_i^{\textrm{new}}) D_{ji}^{-1}({\bf R}^{\textrm{old}})
        \label{slaterRatio}
\end{equation}
where the last equality follows from the definition of the Slater matrix \eqref{slaterMatrix}. 
This operation is simply a dot product of a vector of single-particle wave functions evaluated
at the new position with the $i$-th column of the inverse matrix $D^{-1}$ evaluated at the original position,
and has a time scaling of $\mathcal{O}(N)$.\\

\noindent 

\subsection{Optimization of inverse Slater matrix}
\noindent The operation \eqref{slaterRatio} demands that we maintain the inverse matrix $D^{-1}$ for each
MC cycle. Getting the inverse of an $N\times N$-matrix på Gaussian elimination has a complexicty of order
$\mathcal{O}(N^3)$ operations, which we cannot afford. An alternative way of updating the inverse
of a matrix when only a row/column is changed was suggested by Sherman and Morris. 
This algorithm has a time scaling
of $\mathcal{O}(N^2)$ and is as follows:
\begin{itemize}
 \item Update all but the $i$-th column of $D^{-1}$. For each column $j\neq i$,
 calculate the quantity 
 $$S_j = \sum_{l=1}^N D_{il} ({\bf R}^{\textrm{new}}) D_{lj}^{-1} ({\bf R}^{\textrm{old}})$$
 \item The new elements of the $j$-th column of $D^{-1}$ is then given by: 
 $$D_{kj}^{-1}({\bf R}^{\textrm{new}}) = D_{kj}^{-1}({\bf R}^{\textrm{old}}) - \frac{S_j}{R_{SD}}
 D_{ki}^{-1}({\bf R}^{\textrm{old}}) \quad k = \{1,\dots,N\}, \quad j \neq i$$
 \item Finally the $i$-th column of $D^{-1}$ is updated simply as follows:
 $$D_{ki}^{-1}({\bf R}^{\textrm{new}}) = \frac{1}{R_{SD}} D_{ki}^{-1}({\bf R}^{\textrm{old}}) \quad
 k = \{1,\dots,N\}$$
\end{itemize}
This means that we only need to invert the Slater matrix with conventional methods like
Gaussian elimination or LU-decomposition once, after we have initialized them. For the subesequent steps,
we use the above algorithm to obtain the inverse matrix. 

\subsection{Splitting the Slater determinant}

It can be shown, see for example Moskowitz and Kalos \cite{ref3} that we can approximate the Slater
determinant $|D|$ as a product of two smaller ones, where each can be identified with
spin-up $\uparrow$ and spin-down $\downarrow$ respectively,
\begin{equation}
 |D| = |D|_\uparrow \cdot |D|_\downarrow
\end{equation}
To illustrate, we write out the Slater determinant \eqref{slaterMatrix} for $N=4$:
\begin{equation}
    |D| = \frac{1}{\sqrt{4!}} \left|\begin{array}{cccc}
\phi_{00\uparrow}({\bf r}_1) &\phi_{00\downarrow}({\bf r}_1) 
&\phi_{10\uparrow}({\bf r}_1) &\phi_{10\downarrow}({\bf r}_1) \\
\phi_{00\uparrow}({\bf r}_2) &\phi_{00\downarrow}({\bf r}_2) 
&\phi_{10\uparrow}({\bf r}_2) &\phi_{10\downarrow}({\bf r}_2) \\
\phi_{00\uparrow}({\bf r}_3) &\phi_{00\downarrow}({\bf r}_3) 
&\phi_{10\uparrow}({\bf r}_3) &\phi_{10\downarrow}({\bf r}_3) \\
\phi_{00\uparrow}({\bf r}_4) &\phi_{00\downarrow}({\bf r}_4) 
&\phi_{10\uparrow}({\bf r}_4) &\phi_{10\downarrow}({\bf r}_4) \\
                      \end{array} \right|
\end{equation}
The Slater determinant as written is zero since the spatial wave functions for the
spin-up and spin-down states are equal. 
However,we can now factorize this determinant in the following way,
\begin{equation}
|D| = \frac{1}{\sqrt{2}}\left|\begin{array}{cc}
\phi_{00\uparrow}({\bf r}_1) &\phi_{10\uparrow}({\bf r}_1) \\
\phi_{00\uparrow}({\bf r}_2) &\phi_{10\uparrow}({\bf r}_2) \\
            \end{array} \right|
\cdot
\frac{1}{\sqrt{2}} \left| \begin{array}{cc}
\phi_{00\downarrow}({\bf r}_3) &\phi_{10\downarrow}({\bf r}_3) \\
\phi_{00\downarrow}({\bf r}_4) &\phi_{10\downarrow}({\bf r}_4) \\
       \end{array} \right|
       \label{SlaterDetFactorized}
\end{equation}
This ansatz is not antisymmetric under exchange of two electrons with opposite spins, but it can 
be shown that it gives the same expectation value for the energy as the full Slater determinant. 
The above is correct only for spin-independent Hamiltonians. Our trial wave function
can now be written as
\begin{equation}
 \Psi_T = |D|_\uparrow |D|_\downarrow \Psi_C
 \label{TrialWFSpins}
\end{equation}


\noindent The factorization above makes it possible to perform the calculation of the Slater ratio \eqref{slaterRatio}
and the updating of the inverse Slater matrix seperately for $|D|_\uparrow$ and $|D|_\downarrow$:
\begin{equation}
 \frac{|D|^{\textrm{new}}}{|D|^{\textrm{old}}} = \frac{|D|^{\textrm{new}}_\uparrow}{|D|^{\textrm{old}}_\downarrow}
 \cdot
 \frac{|D|^{\textrm{new}}_\uparrow}{|D|^{\textrm{old}}_\downarrow}
\end{equation}
We see from \eqref{SlaterDetFactorized} that one of the two determinants is unaffected
when we move only one particle at a time, it will therefore cancel from the ratio. This means
that we only need to update one determinant of size $N/2$ when we compute $R_{SD}$
and update $|D|^{-1}$. The efficiency enhancements due to optimization are summed up in Table \ref{tab:optimization}.
\begin{table}[H] 
  \begin{center}
    \begin{tabular*}{14cm}{@{\extracolsep{\fill}} lll}
      \toprule
      Operation & No optimization & With optimization  \\ 
      \hline
      Evaluation of $R_{SD}$  & $\mathcal{O}(N)$ & $\mathcal{O}(N/2)$ \\
      Updating inverse  & $\mathcal{O}(N^2)$ & $\mathcal{O}(N^2/4)$ \\ 
      Transition of one particle & $\mathcal{O}(N)$ + $\mathcal{O}(N^2)$ &
                                   $\mathcal{O}(N/2)$ + $\mathcal{O}(N^2/4)$ \\ 
      \bottomrule
      \end{tabular*} 
    \end{center}
      \caption {Comparison of the computational cost involved in the computation of the
                Slater determinant with and without optimization} 
  \label{tab:optimization} 
\end{table}

\subsection{Optimization of gradient}

We need the ratio $\frac{\nabla \Psi_T}{\Psi_T}$ to compute the quantum force \eqref{quantumForce}
used in importance sampling. This ratio can be written as
\begin{equation}
 \frac{\nabla \Psi_T}{\Psi_T} = \frac{\nabla (\Psi_{SD}\Psi_C)}{\Psi_{SD}\Psi_C} = 
 \frac{\Psi_{C}\nabla\Psi_{SD} + \Psi_{SD}\nabla\Psi_C}{\Psi_{SD}\Psi_C} = 
 \frac{\nabla \Psi_D}{\Psi_D} + \frac{\nabla \Psi_C}{\Psi_C}
\end{equation}
Inserting $\Psi_{SD} = |D|_\uparrow|D|_\downarrow$ yields
\begin{equation}
 \frac{\nabla \Psi_T}{\Psi_T} = \frac{\nabla|D|_\uparrow}{|D|_\uparrow} + 
 \frac{\nabla |D|_\downarrow}{|D|_\downarrow} + \frac{\nabla \Psi_C}{\Psi_C}
 \label{gradientOfTrialWF}
\end{equation}
The process to obtain the gradient-determinant ratio needed in this expression is analogous to
the one used in deriving \eqref{slaterRatio}. The result is
\begin{equation}
 \frac{\nabla_i |D|}{|D|} = \sum_{j=1}^{N/2} \nabla_i D_{ij}({\bf R}^{\textrm{old}}) D_{ji}^{-1}
 ({\bf R}^{\textrm{old}})
        = \sum_{j=1}^{N/2}\nabla_i \phi_j({\bf R}_i^{\textrm{old}}) D_{ji}^{-1}({\bf R}^{\textrm{old}})
        \label{gradientRatioBefore}
\end{equation}
where the sum runs up to $N/2$ for both $|D|_\uparrow$ and $|D|_\downarrow$. 
This expression is used to calculate the quantum force before the ratio \eqref{metropolisRatio} is computed. 
After the position of one particle is altered, the expression is modified to
\begin{equation}
 \frac{\nabla_i |D|}{|D|}
        = \frac{1}{R_{SD}} \sum_{j=1}^{N/2} \nabla_i \phi_j({\bf R}_i^{\textrm{new}}) D_{ji}^{-1}({\bf R}^{\textrm{old}})
        \label{gradientRatioAfter}
\end{equation}
Note that the old inverse Slater matrix is used in both cases because this is not updated until after 
a step has been accepted. We see from \eqref{gradientOfTrialWF} that we can compute
the gradients of the spin-up and spin-down determinants seperately. Furthermore, according to 
the factorization \eqref{SlaterDetFactorized},
one of the Slater gradients is always zero for a given particle $i$:
\begin{itemize}
 \item $\nabla_i |D|_\downarrow = 0$  for $i \leq N/2$
 \item $\nabla_i |D|_\uparrow = 0$ for $i > N/2$
\end{itemize}
thus we only need to compute one of them for each particle. This is implemented as follows:
\belowcaptionskip=-10pt
\begin{lstlisting}[label=slaterGradient,caption=Computation of the Slater gradient ratio for particle i. 
The spin-up matrix only contains the single-particle wave functions as functions of the first half of the 
particles. The spin-down matrix is a function of the second half. One of them is thus
zero for a given particle $i$.]

  // get position of particle i
    double x = particles[i]->getPosition()[0];
    double y = particles[i]->getPosition()[1];

    // spin-up slater
    if (i < m_numberOfParticlesHalf) {
        for (int j=0; j < m_numberOfParticlesHalf; j++) {
            int nx = m_quantumNumbers(j,0);
            int ny = m_quantumNumbers(j,1);
            std::vector<double> grad = singleParticleWFGradient(nx, ny, x, y);
            m_gradientUp[0] += grad[0] * m_slaterSpinUpInverse(j,i);
            m_gradientUp[1] += grad[1] * m_slaterSpinUpInverse(j,i);
        }
    }
    // spin-down slater
    else {
        for (int j=0; j < m_numberOfParticlesHalf; j++) {
            int nx = m_quantumNumbers(j,0);
            int ny = m_quantumNumbers(j,1);
            std::vector<double> grad = singleParticleWFGradient(nx, ny, x, y);
            m_gradientDown[0] += grad[0] * 
            m_slaterSpinDownInverse(j,i-m_numberOfParticlesHalf);
            m_gradientDown[1] += grad[1] * 
            m_slaterSpinDownInverse(j,i-m_numberOfParticlesHalf);
        }
    }
\end{lstlisting}


\subsection{Optimization of the Laplacian}
We need the Laplacian of the trial wave function to compute the expectation value of the kinetic
energy $K$. For electron $i$ this is
\begin{equation}
 \langle K_i \rangle = -\frac{1}{2} \frac{\langle\Psi_T|\nabla_i^2|\Psi_T\rangle}{\langle\Psi_T|\Psi\rangle_T}
 \label{kineticEnergy}
\end{equation}
It can be shown that the Laplacian of the trial wave function can be written
\begin{equation}
 \frac{\nabla^2\Psi_T}{\Psi_T} = \frac{\nabla^2|D|_\uparrow}{|D|_\uparrow} + 
 \frac{\nabla^2|D|_\downarrow}{|D|_\downarrow} + \frac{\nabla^2\Psi_C}{\Psi_C} + 
 2\left[\frac{\nabla|D|_\uparrow}{|D|_\uparrow} + \frac{\nabla|D|_\downarrow}{|D|_\downarrow}\right]
 \cdot \frac{\nabla \Psi_C}{\Psi_C}
\end{equation}
where the Laplace-determinant-to-determinant ratio is given by
\begin{equation}
 \frac{\nabla_i^2|D|}{|D|} = \sum_{j=1}^{N/2}\nabla_i^2D_{ij}({\bf R}^{\textrm{new}})D_{ji}^{-1}({\bf R}^{\textrm{new}})
 = \sum_{j=1}^{N/2}\nabla_i^2\phi_j({\bf R}_i^{\textrm{new}})D_{ji}^{-1}({\bf R}^{\textrm{new}})
\end{equation}
This expression must be computed for all electrons to obtain the total Laplacian for the whole system, which is needed
in the local energy \eqref{localEnergy}. To save computation time, we only recalculate $E_L$ when
a step is accepted. 

\subsection{Optimization of the correlation-to-correlation ratio}
The correlation function $\Psi_C$ \eqref{correlationFunction} can be written
\begin{equation}
 \Psi_C = \prod_{i<j}^N\exp{(f_{ij})} = \exp(U)
\end{equation}
where 
\begin{equation}
 f_{ij} = \frac{a_{ij}r_{ij}}{1+\beta r_{ij}}
 \label{pade-jastrow}
\end{equation}
and 
\begin{equation}
 U = \sum_{i<j}^Nf_{ij}
\end{equation}
We see that $\Psi_C$ depends on the relative distances $r_{ij}$ between the electrons. The total
number of different relative distances is $N(N-1)/2$, computing the correlation-to-correation ratio
$R_C$ therefore scales as $\mathcal{O}(N^2)$. However, when moving only one electron at a time, say the 
$k$-th electron, only the $N-1$ distances having $k$ as one of their indicies are changed. The 
rest of the factors thus cancel, and we have
\begin{equation}
 R_C = \frac{\Psi_C^{\textrm{new}}}{\Psi_C^{\textrm{old}}} = 
 \frac{\exp{(U^{\textrm{new}})}}{\exp{(U^{\textrm{old}})}} = 
 \exp{(\Delta U)}
\end{equation}
where 
\begin{equation}
 \Delta U = \sum_{i\neq k}^N (f_{ki}^{\textrm{new}} - f_{ki}^{\textrm{old}})
\end{equation}

\subsection{Optimization of the $\nabla \Psi_C / \Psi_C$ ratio}
The expression to be derived in the following is of interest when computing the quantum force \eqref{quantumForce}
and the kinetic energy \eqref{kineticEnergy}. From the discussion in the last section, only $N-1$ terms survive
when we differentiate w.r.t. particle $k$. 
The ratio $\nabla \Psi_C / \Psi_C$ for particle $i$
in one dimension is thus
\begin{equation}
\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k} = 
\frac{1}{\exp{U}}\frac{\partial \exp{U}}{\partial x_k} = 
\frac{\partial U}{\partial x_k} = 
\sum_{i\neq k}^N \frac{\partial f_{ki}}{\partial x_k}
\end{equation}
where
\begin{equation}
 \frac{\partial f_{ki}}{\partial x_k} = \frac{\partial f_{ki}}{\partial r_{ki}}
 \frac{\partial r_{ki}}{\partial x_k} = \frac{a_{ki}}{(1 + \beta r_{ki})^2}\frac{x_k - x_i}{r_{ki}}
\end{equation}
with similar expressions for $y_k$. 

\subsection{Optimization of the $\nabla^2 \Psi_C / \Psi_C$ ratio}
The following ratio is needed to compute the Laplacian $\nabla^2 \Psi_T / \Psi_T$. 
For particle $k$ in one dimension this is
\begin{align}
 \frac{1}{\Psi_C}\frac{\partial^2 \Psi_C}{\partial x_k^2} &= 
 \frac{1}{\exp{U}}\frac{\partial}{\partial x_k}\left(\frac{\partial \exp{U}}{\partial x_k}\right) = 
 \frac{1}{\exp{U}}\frac{\partial}{\partial x_k}\left(\exp{U}\frac{\partial \exp{U}}{\partial x_k}\right)\\ &= 
 \left(\frac{\partial U}{\partial x_k}\right)^2 + \frac{\partial^2 U}{\partial x_k^2} =
 \left(\frac{1}{\Psi_C}\frac{\partial \Psi_C}{\partial x_k}\right)^2 + 
 \sum_{i\neq k}^N \frac{\partial^2 f_{ki}}{\partial x_k^2}
\end{align}
The total Laplacian for particle $k$ is thus
\begin{equation}
 \frac{\nabla^2_k \Psi_C}{\Psi_C} = \left(\frac{\nabla_k \Psi_C}{\Psi_C}\right)^2 + 
 \sum_{i\neq k}^N \nabla_i^2 f_{ki}
\end{equation}
where
\begin{equation}
 \nabla_i^2 f_{ki}  = 
 \frac{1}{r_{ki}}\frac{\partial f_{ki}}{\partial r_{ki}} + \frac{\partial^2 f_{ki}}{\partial r_{ki}^2}
\end{equation}
and 
\begin{equation}
 \frac{\partial^2 f_{ki}}{\partial r_{ki}^2} = -\frac{2a_{ki}\beta}{(1 + \beta r_{ki})^3}
\end{equation}
We must sum over all particles $k$ and both dimensions to obtain the full Laplacian for the system.



\section{Implementation}

We have made an object-oriented code in C++. We give here 
an overview of the class structure and what the different classes do,
\begin{itemize}
 \item \textit{Main program}: Sets all the parameters needed to a simulation. 
 \item \textit{System}: Runs the Monte Carlo cycles with/without importance sampling
 \item \textit{Sampler}: Samples quantities we want to measure for each cycle and computes expectation values
 \item \textit{Particle}: Sets and adjusts particle positions
 \item \textit{SteepestDescent}: Runs the steepest descent method
 \item \textit{InitialState}: Super-class for setting up different initial states
 \begin{itemize}
     \item \textit{RandomUniform}: Assigns initial positions according to a uniform distribution
 \end{itemize}
 \item \textit{WaveFunction}: Super-class for different wave functions. Sets the variational parameters.
       All subclasses must implement functions to evalute the analytical expressions for $\Psi_T$, 
       $\nabla \Psi_T$, $\nabla^2 \Psi_T$ and $d\Psi_T / d\alpha$.
 \begin{itemize}
     \item \textit{SimpleGaussian}: Implements the above quantities for $\Psi_T$ used in system 1
     \item \textit{InteractingGaussian}: Implements the above quantities for $\Psi_T$ used in system 2
 \end{itemize}
     \item \textit{Hamiltonian}: Super-class for different Hamiltonians. Calculates $E_L$ by computing
       potential and kinetic energy, either numerically or analytically for the latter. The analytical
       Laplacian is obtained from \textit{WaveFunction}.
  \begin{itemize}
       \item \textit{HarmonicOscillator}: Hamiltonian for system 1
       \item \textit{HarmonicOscillatorInteracting}: Hamiltonian for system 2
  \end{itemize}
\end{itemize}
In addition to these we use class \textit{Random} to generate pseudo-random numbers. \\

\noindent All the information are stored in System in the form of class objects of the other classes, which in turn 
recieves the System object so that they can access this information via
setters and getters in the System class. This way of communicating between classes limits the user's 
capability to alter vital functionality and also makes the program more user-friendly. 
Object-oriented code is also easy to expand on. We don't need to add new functionality to
e.g. implement a new wave function; only the specifics of this new wave function needs to be implemented. 

\section{Results and discussion}


\subsection{Some text}

\subsection{More than two particles}

\subsection{Performance analysis}


\begin{table}[H]
\caption{Optimal $\alpha$ and  $\beta$ values for given $\omega$'s  with jastrow factor.}
\centering
\begin{tabular}{ c c c }
\hline
\hline
  $\omega$ & $\alpha$ & $\beta$ \\
  \hline
1    & 0.992067 & 0.400016\\
0.5  & 0.952981 & 0.354743\\
0.1  & 0.952833 & 0.354292\\
0.05 & 0.913976 & 0.235196\\
0.01 & 0.911692 & 0.203919\\
\hline
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\caption{Optimal $\alpha$ values for given $\omega$'s  without jastrow factor.}
\centering
\begin{tabular}{ c c }
\hline
\hline
  $\omega$ & $\alpha$ \\
  \hline
1    & 0.686717\\
0.5  & 0.669455\\
0.1  & 0.655921\\
0.05 & 0.526488\\
0.01 & 0.477602\\
\hline
\hline
\end{tabular}
\end{table}


\begin{table}[H]
\caption{Optimal $\alpha$ and $\beta$ values for given $\omega$'s  with jastrow factor and Slater determinant.}
\centering
\begin{tabular}{ c c c c }
\hline
\hline
 N & $\omega$ & $\alpha$ & $\beta$ \\
\hline
 6  & 1    & 1.03741   & 0.472513\\
   & 0.5  & 0.931202  & 0.395044\\
   & 0.1  & 0.831104  & 0.211443\\
   & 0.05 & 0.849742  & 0.155566\\
   & 0.01 & 0.842683  & 0.10789\\
 \hline 
12 & 1    & 1.10364   & 0.468861\\
   & 0.5  & 0.936306  & 0.414534\\
   & 0.1  & 0.84105   & 0.208143 \\
   & 0.05 & 0.842746  & 0.153921\\
   & 0.01 & 0.835467  & 0.0923274\\
   \hline
20 & 1    & 1.06019   & 0.474467\\
   & 0.5  & 0.944121  & 0.419367\\
   & 0.1  & 0.856981  & 0.200372\\
   & 0.05 & 0.842341  & 0.153017\\
   & 0.01 & 0.835301  & 0.0922157\\
   \hline
   \hline
\end{tabular}
\end{table}


\begin{table}[H]
\caption{Optimal $\alpha$ values for given $\omega$'s with Slater determinant and without jastrow factor}
\centering
\begin{tabular}{ c c c  }
\hline
\hline
N & $\omega$ & $\alpha$  \\
\hline
6  & 1    & 1.03741 \\  
   & 0.5  & 0.931202  \\
   & 0.1  & 0.831104  \\
   & 0.05 & 0.849742  \\
   & 0.01 & 0.842683  \\
\hline  
12 & 1    & 1.10364   \\
   & 0.5  & 0.936306  \\
   & 0.1  & 0.84105   \\
   & 0.05 & 0.842746  \\
   & 0.01 & 0.835467  \\
\hline   
20 & 1    & 1.06019   \\
   & 0.5  & 0.944121  \\
   & 0.1  & 0.856981  \\
   & 0.05 & 0.842341  \\
   & 0.01 & 0.835301  \\
\hline
\hline
   \end{tabular}
\end{table}

\begin{figure}[H]
\begin{center}

\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_w=1_N=2_WithoutSlater.png}
  \caption{$\omega = 1$ and $N = 2$ without Slater determinant.}
  \label{fig:NAVN1}
\end{subfigure}
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_w=05_N=2_WithoutSlater.png}
  \caption{$\omega = 0.5$ and $N = 2$ without Slater determinant.}
  \label{fig:NAVN2}
\end{subfigure}
\vspace{1cm}
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_w=01_N=2_WithoutSlater.png}
  \caption{$\omega = 0.1$ and $N = 2$ without Slater determinant.}
  \label{fig:NAVN3}
\end{subfigure}
   \begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_w=005_N=2_WithoutSlater.png}
  \caption{$\omega = 0.05$ and $N = 2$ without Slater determinant.}
  \label{fig:NAVN4}
\end{subfigure}
\vspace{1cm}
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_w=001_N=2_WithoutSlater.png}
  \caption{$\omega = 0.01$ and $N = 2$ without Slater determinant.}
  \label{fig:NAVN5}
\end{subfigure}
    \label{WithoutSlater}
  \end{center}
  \caption{blablabla}
\end{figure}

\begin{figure}[H]
\begin{center}
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_N=6_w=1_Slater.png}
  \caption{$\omega = 1$ and $N = 6$ with Slater determinant.}
  \label{fig:NAVN1}
\end{subfigure}
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_N=6_w=05_Slater.png}
  \caption{$\omega = 0.5$ and $N = 6$ with Slater determinant.}
  \label{fig:NAVN2}
\end{subfigure}
\vspace{1cm}
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_N=6_w=01_Slater.png}
  \caption{$\omega = 0.1$ and $N = 6$ with Slater determinant.}
  \label{fig:NAVN3}
\end{subfigure}
   \begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=1.1\linewidth]{OBD_N=6_w=005_Slater.png}
  \caption{$\omega = 0.05$ and $N = 6$ with Slater determinant.}
  \label{fig:NAVN4}
\end{subfigure}
\vspace{1cm}
%\begin{subfigure}{.4\textwidth}
%  \centering
%  \includegraphics[width=1.1\linewidth]{OBD_N=6_w=001_Slater.png}
%  \caption{$\omega = 0.01$ and $N = 6$ with Slater determinant.}
%  \label{fig:NAVN5}
%\end{subfigure}
    \label{WithoutSlater}
  \end{center}
  \caption{blablabla}
\end{figure}


\section{Conclusions}
 



\newpage 
%\section{Appendix}
\appendix
\section{Appendix}
\subsection{Analytic local energy for two-body quantum dot} \label{app:appendixA1}

\begin{align}
 \Psi_T({\bf r}_1, {\bf r}_2) &= \textrm{exp}(-\alpha \omega(r_1^2 + r_2^2)/2)
 \textrm{exp}\left(\frac{a r_{12}}{1 + \beta r_{12}}\right) \\
 &= K_1K_2
\end{align}
Laplacian of this wave function:
\begin{equation}
 \nabla^2 \Psi_T({\bf r}_1, {\bf r}_2) = \frac{\partial \Psi_T}{\partial x_1^2} + 
 \frac{\partial \Psi_T}{\partial y_1^2} + \frac{\partial \Psi_T}{\partial x_2^2} +
 \frac{\partial \Psi_T}{\partial y_2^2}
\end{equation}
Using the following
\begin{equation},
 \frac{\partial r_1}{\partial x_1} = x_1/r_1
\end{equation}
and 
\begin{equation}
 \frac{\partial r_{12}}{\partial x_1} = (x_1 - x_2)/r_1
\end{equation}
Gradient of first term:
\begin{equation}
 \frac{\partial K_1}{\partial x_1} = -\alpha \omega x_1 K_1
\end{equation}
so that
\begin{equation}
 \nabla K_1 = -\alpha\omega K_1 {\bf r}
\end{equation}
where
\begin{equation}
 {\bf r} = (x_1, y_1, x_2, y_2)
\end{equation}
Gradient of second term:
\begin{equation}
 \frac{\partial K_2}{\partial x_1} = K_2 \frac{a(x_1 - x_2)}{r_{12}(1 + \beta r_{12})^2}
\end{equation}
so that
\begin{equation}
 \nabla K_2 = 
 K_2 \frac{a}{r_{12}(1 + \beta r_{12})^2} {\bf r}_{12}
\end{equation}
where
\begin{equation}
 {\bf r}_{12} = (x_1 - x_2, y_1 - y_2, x_2 - x_1, y_2 - y_1) 
\end{equation}
The quantum force used in importance sampling is thus
\begin{align}
 F = 2\frac{\nabla \Psi_T}{\Psi_T} = 2\frac{\nabla(K_1K_2)}{K_1K_2} = 
 2\frac{1}{K_1K_2}(K_2 \nabla K_1 + K_1 \nabla K_2) = 
 -2\alpha\omega {\bf r} + \frac{2a}{r_{12}(1 + \beta r_{12})^2} {\bf r}_{12}
\end{align}
Laplacian of first term:
\begin{equation}
 \frac{\partial^2 K_1}{\partial x_1^2} = K_1 (\alpha^2 \omega^2 x_1^2 -\alpha \omega )  
\end{equation}
so that
\begin{equation}
 \nabla^2 K_1 = K_1 (\alpha^2 \omega^2 (r_1^2 + r_2^2) - 4\alpha \omega)
\end{equation}
Laplacian of second term:
\begin{align}
\begin{split}
 \frac{\partial^2 K_2}{\partial x_1^2} = K_2 \Biggr[&\frac{a^2(x_1-x_2)^2}{r_{12}^2(1 + \beta r_{12})^4} +
 \frac{ar_{12}(1 + \beta r_{12})^2}{r_{12}^2(1 + \beta r_{12})^4} \\
 &-\frac{a(x_1 - x_2)[(x_1-x_2)/r_{12} (1 + \beta r_{12})^2 + 2r_{12}(1 + \beta r_{12})\beta(x_1-x_2)/r_{12}]}
 {r_{12}^2(1 + \beta r_{12})^4} \Biggr]
\end{split}
\end{align}
thus
\begin{align}
\begin{split}
 \frac{\partial^2 K_2}{\partial x_1^2} = K_2 \Biggr[\frac{a^2(x_1-x_2)^2}{r_{12}^2(1 + \beta r_{12})^4} +
 \frac{a}{r_{12}(1 + \beta r_{12})^2} 
 -\frac{a(x_1 - x_2)^2}{r_{12}^3(1 + \beta r_{12})^2}
 - \frac{2a\beta(x_1 - x_2)^2} {r_{12}^2(1 + \beta r_{12})^3} \Biggr]
\end{split}
\end{align}
so that
\begin{align}
\begin{split}
 \nabla^2 K_2 &= K_2\Biggr[\frac{2a^2}{(1 + \beta r_{12})^4} + \frac{4a}{r_{12}(1 + \beta r_{12})^2}
              - \frac{2a}{r_{12}(1 + \beta r_{12})^2} - \frac{2a\beta}{(1 + \beta r_{12})^3} \Biggr] \\
              &= K_2\frac{2a}{(1+\beta r_{12})^2}\Biggr[ \frac{a}{(1+\beta r_{12})^2} + 
              \frac{1}{r_{12}} - \frac{2\beta}{1 + \beta r_{12}} \Biggr]
\end{split}
\end{align}
Have that
\begin{equation}
 \nabla^2 \Psi_T = \nabla^2K_1K_2 + 2\nabla K_1\nabla K_2 + K_1\nabla^2K_2
\end{equation}
and
\begin{align}
\begin{split}
 \nabla K_1 \nabla K_2 &= -K_1K_2 \frac{a\alpha \omega}{r_{12}(1+\beta r_{12})^2}
 \Bigr[x_1(x_1 - x_2) + y_1(y_1 - y_2) - x_2(x_1 - x_2) - y_2(y_1 - y_2)\Bigr] \\
 &= -K_1K_2 \frac{a\alpha \omega}{r_{12}(1+\beta r_{12})^2}
 \Bigr[(x_1 - x_2)(x_1 - x_2) + (y_1 - y_2)(y_1 - y_2)\Bigr] \\
 &= -K_1K_2 \frac{a\alpha \omega r_{12}}{(1+\beta r_{12})^2}
\end{split}
\end{align}
The analytic expression for the Laplacian ratio is thus
\begin{align}
\begin{split}
 \frac{\nabla^2\Psi_T}{\Psi_T} &= 2\alpha^2 \omega^2(r_1^2 + r_2^2) - 4\alpha \omega
 - \frac{2a\alpha \omega r_{12}}{(1+\beta r_{12})^2} \\
 &+ \frac{2a}{(1+\beta r_{12})^2}\Biggr[ \frac{a}{(1+\beta r_{12})^2} + 
              \frac{1}{r_{12}} - \frac{2\beta}{1 + \beta r_{12}} \Biggr]
              \label{analyticLocalEnergy}
\end{split}
\end{align}
and we get for the local energy in the Hamiltonian \eqref{fullHamiltonian}
\begin{equation}
 E_L = \frac{1}{\Psi_T}H\Psi_T = -\frac{1}{2}\frac{\nabla^2\Psi_T}{\Psi_T} + 
 \frac{1}{2}\omega^2(r_1^2 + r_2^2) + \frac{1}{r_{12}}
\end{equation}


\subsection{Single-particle wave functions and their derivatives} \label{app:appendixA2}
The Slater matrix is a function of the single-particle wave functions
\begin{equation}
 \phi_{n_x,n_y}(x,y) = AH_{n_x}(\sqrt{\alpha\omega}x)
 H_{n_y}(\sqrt{\alpha\omega}y)\exp(-\alpha\omega(x^2+y^2)/2).
\end{equation}
We need the gradient and Laplacian of these functions to compute $\nabla |D| / |D|$ 
and $\nabla^2 |D| / |D|$. The two components of the gradient w.r.t to particle $i$ is
\begin{equation}
 \frac{\phi_{n_x,n_y}(x_i,y_i)}{\partial x_i} = \exp{[-\alpha\omega(x_i^2 + y_i^2)/2]}H_{n_y}(\sqrt{\alpha\omega}y_i)
 \left( \frac{\partial H_{n_x}(\sqrt{\alpha\omega}x_i)}{\partial x_i} - 
 H_{n_x}(\sqrt{\alpha\omega}x_i)\alpha\omega x_i \right)
\end{equation}
and 
\begin{equation}
 \frac{\partial \phi_{n_x,n_y}(x_i,y_i)}{\partial y_i} = \exp{[-\alpha\omega(x_i^2 + y_i^2)/2]}
 H_{n_x}(\sqrt{\alpha\omega}x_i)
 \left( \frac{\partial H_{n_y}(\sqrt{\alpha\omega}y_i)}{\partial y_i} - 
 H_{n_y}(\sqrt{\alpha\omega}y_i)\alpha\omega y_i \right)
\end{equation}
and the Laplacian is given as
\begin{align}
\begin{split}
 \nabla^2\phi_{n_x,n_y}(x_i,y_i) = &\exp{[-\alpha\omega(x_i^2 + y_i^2)/2]} \\
 \Biggr(&H_{n_y}(\sqrt{\alpha\omega}y_i) \frac{\partial^2 H_{n_x}(\sqrt{\alpha\omega}x_i)}{\partial x_i^2} + 
 H_{n_x}(\sqrt{\alpha\omega}x_i)\frac{\partial^2 H_{n_y}(\sqrt{\alpha\omega}y_i)}{\partial y_i^2} \quad+ \\
 &\alpha\omega H_{n_x}H_{n_y}(\alpha\omega(x_i^2 + y_i^2) - 2)\quad - \\
 &2\alpha\omega x H_{n_y}\frac{\partial H_{n_x}(\sqrt{\alpha\omega}x_i)}{\partial x_i} - 
 2\alpha\omega y H_{n_x}\frac{\partial H_{n_y}(\sqrt{\alpha\omega}y_i)}{\partial y_i} \Biggr)
\end{split}
\end{align}
We also need the derivatives w.r.t. the variational parameters for optimizing:
\begin{align}
\begin{split}
 \frac{\partial \phi_{n_x,n_y}(x_i, y_i)}{\partial \alpha} = 
 &\exp[-\alpha\omega(x_i^2 + y_i^2)/2] \\
 \Biggr(&H_{n_x}(\sqrt{\alpha\omega}x_i)\frac{\partial H_{n_y}(\sqrt{\alpha\omega}y_i)}{\partial \alpha} + 
 H_{n_y}(\sqrt{\alpha\omega}y_i)\frac{\partial H_{n_x}(\sqrt{\alpha\omega}x_i)}{\partial \alpha} \quad- \\
 &\frac{1}{2}\omega H_{n_x}(\sqrt{\alpha\omega}x_i)H_{n_y}(\sqrt{\alpha\omega}y_i)(x_i^2 + y_i^2)\Biggr)
\end{split}
\end{align}
and 
\begin{equation}
 \frac{\partial \phi_{n_x,n_y}(x_i, y_i)}{\partial \beta} = 0
\end{equation}


\subsection{Hermite polynomials and their derivatives} \label{app:appendixA3}
The Hermite polynomials are the solutions of the following differential
equation
\begin{equation}
   \frac{d^2H(x)}{dx^2}-2x\frac{dH(x)}{dx}+(\lambda-1)H(x)=0.
   \label{eq:hermite}
\end{equation}
We need the first four polynomials in this project,
\begin{align}
\begin{split}
   &H_0(x)=1 \\
   &H_1(x)=2x \\
   &H_2(x)=4x^2-2 \\
   &H_3(x)=8x^3-12x \\
   \label{hermite}
\end{split}
\end{align}
We list here the different derivatives of $H_{n_x}(\sqrt{\alpha\omega} x_i)$ .
The corresponding relations for $H_{n_y}(\sqrt{\alpha\omega} y_i)$ can be obtained by
substituting $x_i \rightarrow y_i$. 
\begin{align}
\begin{split}
\frac{\partial H_0(\sqrt{\alpha\omega} x_i)}{\partial x_i} &= 0 \\
\frac{\partial H_1(\sqrt{\alpha\omega} x_i)}{\partial x_i} &= 2\sqrt{\alpha\omega} \\
\frac{\partial H_2(\sqrt{\alpha\omega} x_i)}{\partial x_i} &= 8\alpha\omega x_i \\
\frac{\partial H_3(\sqrt{\alpha\omega} x_i)}{\partial x_i} &= 24(\alpha\omega)^{3/2}x_i^2 - 12\sqrt{\alpha\omega}
\end{split}
\end{align}
and 
\begin{align}
\begin{split}
\frac{\partial^2 H_0(\sqrt{\alpha\omega} x_i)}{\partial x_i^2} &= 0 \\
\frac{\partial^2 H_1(\sqrt{\alpha\omega} x_i)}{\partial x_i^2} &= 0 \\
\frac{\partial^2 H_2(\sqrt{\alpha\omega} x_i)}{\partial x_i^2} &= 8\alpha\omega \\
\frac{\partial^2 H_3(\sqrt{\alpha\omega} x_i)}{\partial x_i^2} &= 48(\alpha\omega)^{3/2}x_i
\end{split}
\end{align}
and 
\begin{align}
\begin{split}
&\frac{\partial H_0(\sqrt{\alpha\omega} x_i)}{\partial \alpha} = 0 \\
&\frac{\partial H_1(\sqrt{\alpha\omega} x_i)}{\partial \alpha} = \sqrt{\omega/\alpha}x_i \\
&\frac{\partial H_2(\sqrt{\alpha\omega} x_i)}{\partial \alpha} = 4\omega x_i^2 \\
&\frac{\partial H_3(\sqrt{\alpha\omega} x_i)}{\partial \alpha} = 12 \sqrt{\alpha}\omega^{3/2} x_i^3
 - 6 \sqrt{\omega/\alpha}x_i
\end{split}
\end{align}











\newpage

\begin{thebibliography}{9}

%\bibitem{ref1}
%  J. K. Nilsen, J. Mur-Petit, M. Guilleumas, M. Hjorth-Jensen and A. Polls, 
% \textit{Vortices in atomic Bose-Einstein condensates in the large-gas-parameter region}, 
%  Phys. Rev. A {\bf 71}, 053610 (2005).

\bibitem{ref1}
 M. Taut, 
 \textit{Two electrons in an external oscillator potential: Particular analytic solutions
 of a Coulomb interaction problem}
 Phys. Rev. A {\bf 48}, 3561 - 3566 (1993).
 
 \bibitem{ref2}
 M. L. Pedersen, G. Hagen, M. Hjorth-Jensen, S. Kvaal, and F. Pederiva, 
 \textit{Ab inito computation of the energies of circular quantum dots}
 Phys. Rev. B {\bf 84}, 115302 (2011)
 
 \bibitem{ref3}
 Jules W. Moskowitz, M. H. Kalos,
 \textit{A new look at correlations in atomic and molecular systems. 
         I. Application of fermion monte carlo variational method.}
 Int. J. Quantum Chem. {\bf 20} 1107 (1981)

\end{thebibliography}

\end{document}

\begin{comment}

% deloppgave
\begin{enumerate}
\item[\bf a)]
\item[\bf b)]
\item[\bf c)]
\item[\bf d)]
\end{enumerate}

%%%%%%%%
% Tabell
\begin{table}[H]
  \centering
  \begin{tabular}{ | c | r | r | r | r | r |}
    \hline
    & & & & & \\*
    \hline
    & & & & & \\*
    \hline
  \end{tabular}
  \caption{some caption}
  \label{tab:Tabell1}
\end{table}

%%%%%%%%
% Enkel figur
\begin{figure}[H]
\begin{center}
  \includegraphics[width = 120mm]{}
  \caption{some caption}\label{fig:fig1}
  \end{center}
\end{figure}

%%%%%%%%
% 2 figurer sbs
\begin{figure}
\begin{minipage}[t]{0.48\linewidth}
  \includegraphics[width=\textwidth]{fil}
  \caption{}
  \label{fig:minipage1}
\end{minipage}
\quad
\begin{minipage}[t]{0.48\linewidth}
\includegraphics[width=\textwidth]{fil}
  \caption{}
  \label{fig:minipage1}
\end{minipage}
\end{figure}

%%%%%%%%
% X antall kollonner
\begin{multicols*}{X}
\begin{spacing}{0.7} % verticale mellomrom
%kan f.eks benytte align?
\end{spacing}
\end{multicols*}


%%%%%%%%
%Matrise
\begin{equation*}
    {\bf A} = \left(\begin{array}{cccccc}
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                           z &z &z &z &z &z \\
                      \end{array} \right)
\end{equation*}
%%%%%%%%
\end{comment}
